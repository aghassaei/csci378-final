{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineRenderer.figure_format = 'retina'\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import random\n",
    "import torch.utils.tensorboard as tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Environment\n",
    "\n",
    "The game environment `GameEnv` is formally given by \n",
    "\n",
    "**State space:** $14$ since there are $14$ holes on the board\n",
    "\n",
    "**Action space:** $6$ since there are $6$ holes a player can choose to redistribute\n",
    "\n",
    "**Reward function:** TODO\n",
    "\n",
    "\n",
    "The limit of the trajectories is 200 moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv:\n",
    "    def __init__(self, limit):\n",
    "        self.state = None\n",
    "        self.count = 0\n",
    "        self.limit = limit\n",
    "        self.state_dim = 14\n",
    "        self.action_dim = 6\n",
    "        self.rng = np.random.default_rng()\n",
    "     \n",
    "\n",
    "    def reset(self, state=np.array([4,4,4,4,4,4,0,4,4,4,4,4,4,0])):\n",
    "        self.state = state\n",
    "        self.count = 0\n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, a):\n",
    "        '''Parameters:\n",
    "            a (int) : the action the learner will take'''\n",
    "\n",
    "        self.count += 1\n",
    "\n",
    "        # not saving the start state\n",
    "\n",
    "        # Keep track of the state before the learner moves\n",
    "        sstate = self.get_state()\n",
    "\n",
    "        # Intermediate state the state the the learner will modify\n",
    "        istate = self.get_state()\n",
    "\n",
    "        # pieces the learner will move\n",
    "        pieces = istate[a]\n",
    "\n",
    "        # set the state index to 0\n",
    "        istate[a] = 0\n",
    "\n",
    "        # First index to place pieces in \n",
    "        index = (a + 1) % 13\n",
    "        \n",
    "        # Distribute pieces\n",
    "        while pieces > 0:\n",
    "            istate[index] += 1\n",
    "            index = (index + 1) % 13\n",
    "            pieces -= 1\n",
    "            # print(pieces)\n",
    "        \n",
    "        # end state\n",
    "        estate = istate\n",
    "\n",
    "        # Random opponent\n",
    "        # Pick random action\n",
    "        rand_a = self.rng.integers(7, 13)\n",
    "        rand_pieces = estate[rand_a]\n",
    "        estate[rand_a] = 0\n",
    "\n",
    "        rand_index = (rand_a + 1) % 14\n",
    "\n",
    "        while rand_pieces > 0:\n",
    "            if rand_index != 6:\n",
    "                estate[rand_index] += 1\n",
    "                rand_pieces = rand_pieces - 1\n",
    "\n",
    "            rand_index = (rand_index + 1) % 14\n",
    "                \n",
    "\n",
    "        terminated = (estate[6] > 24) or (estate[13] > 24)\n",
    "        truncated = self.count > self.limit\n",
    "\n",
    "        # Reward every time learner is ahead\n",
    "        if estate[6] > estate[13]:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        \n",
    "        self.state = estate\n",
    "\n",
    "        return sstate, istate, self.state, reward, terminated, truncated, a, rand_a\n",
    "        \n",
    "        \n",
    "        \n",
    "        # return self.end_state()\n",
    "            \n",
    "\n",
    "        # no going again\n",
    "        # if index == 6:\n",
    "        #     return self.end_state()\n",
    "        \n",
    "\n",
    "\n",
    "        # for i in range(a + 1, a + pieces):\n",
    "        #     # print(\"pieces left:\", k)\n",
    "        #     if i == 6:\n",
    "        #         continue\n",
    "\n",
    "        #     self.state[k] += 1\n",
    "      \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # # print(\"LEARNER'S TURN\")\n",
    "        # assert(type(self.state) == np.ndarray) \n",
    "        \n",
    "        # # Distribute pieces and save the index of the last piece \n",
    "        # # last_idx = self.distribute(action + 1, pieces, 6)\n",
    "        \n",
    "        # assert(type(self.state) == np.ndarray) \n",
    "        # if not (np.sum(self.state) == 48):\n",
    "\n",
    "        #     assert(False)\n",
    "        \n",
    "\n",
    "        # # Instead of going again, we just skip the opponent's turn\n",
    "        # if last_idx == 6:\n",
    "        #     # print(\"Early return\")\n",
    "        #     assert(type(self.state) == np.ndarray) \n",
    "        #     return self.end_state()\n",
    "        \n",
    "        # assert(type(self.state) == np.ndarray) \n",
    "        # # Capture\n",
    "        # # If the last index only has one piece\n",
    "        # if self.state[last_idx] == 1:\n",
    "        #     # assert(np.sum(self.state) == 48)\n",
    "        #     assert(type(self.state) == np.ndarray) \n",
    "        #     self.capture(last_idx, 6)\n",
    "        #     assert(type(self.state) == np.ndarray) \n",
    "        #     # assert(np.sum(self.state) == 48)\n",
    "\n",
    "        # Opponent's turn\n",
    "        # Just updates the state, side effects only\n",
    "        # print(\"OPPONENT'S TURN\")\n",
    "        # assert(type(self.state) == np.ndarray) \n",
    "        # assert(np.sum(self.state) == 48)\n",
    "        \n",
    "        # self.rand_agent(13)\n",
    "        # assert(np.sum(self.state) == 48)\n",
    "        \n",
    "        # assert(np.sum(self.state) == 48)\n",
    "        \n",
    "    \n",
    "    # def end_state(self):\n",
    "        \n",
    "    # # Game terminates if one player has the majority of pieces in their store\n",
    "    #     terminated = self.state[6] > 24 or self.state[13] > 24\n",
    "    #     truncated = self.count > self.limit\n",
    "\n",
    "    #     # Reward every time learner is ahead\n",
    "    #     if self.state[6] > self.state[13]:\n",
    "    #         reward = 1\n",
    "    #     else:\n",
    "    #         reward = 0\n",
    "        \n",
    "    #     # assert(np.sum(self.state) == 48)\n",
    "    #     return self.state, reward, terminated, truncated\n",
    "\n",
    "    \n",
    "    # def distribute(self, a, pieces, exclude):\n",
    "    #     '''\n",
    "    #     Parameters:\n",
    "    #     a (int) : the action taken (index of hole to redistribute)\n",
    "    #     '''\n",
    "    #     # print(np.sum(self.state))\n",
    "    #     # assert(type(self.state) == np.ndarray)\n",
    "    #     # assert((np.sum(self.state) + pieces) == 48)\n",
    "        \n",
    "    #     # print(\"Distributing pieces\")\n",
    "\n",
    "\n",
    "    \n",
    "    # def capture(self, i, store):\n",
    "    #     assert(type(self.state) == np.ndarray) \n",
    "    #     adj = 12 - i\n",
    "    #     self.state[store] += self.state[adj]\n",
    "    #     self.state[adj] = 0\n",
    "        \n",
    "    \n",
    "    # def rand_agent(self, store):\n",
    "    #     assert(type(self.state) == np.ndarray) \n",
    "    #     while True:\n",
    "            \n",
    "    #         action = self.rng.integers(7, 13)\n",
    "    #         pieces = self.state[action]\n",
    "\n",
    "    #         self.state[action] = 0\n",
    "\n",
    "    #         # print('store =',store)\n",
    "    #         last_idx = self.distribute(action + 1, pieces, store)\n",
    "\n",
    "    #         if last_idx != 13:\n",
    "    #             return\n",
    "            \n",
    "    #         if self.state[last_idx] == 1:\n",
    "    #             self.state = self.capture(self.state, last_idx, store)\n",
    "\n",
    "\n",
    "    # @staticmethod\n",
    "    # def format(x):\n",
    "    #     if x<10: \n",
    "    #         return \"0\" + str(x)\n",
    "    #     return str(x)\n",
    "    \n",
    "    # def view(self):\n",
    "    #     print(\"Board:\", self.state)\n",
    "    #     print(\"Pieces on board:\", np.sum(self.state))\n",
    "    #     print(\"Count (self):\", self.count)\n",
    "        # output = \" -----------------------------------\\n|  |\"\n",
    "        # for i in range(12,6, -1): \n",
    "        #     output.join([\" \", format(self.self.state[i]), \" |\"])\n",
    "        \n",
    "        # output.join([\"|  |\\n\"])\n",
    "\n",
    "        # for i in range(6): \n",
    "        #     output.join([\" \", format(self.self.state[i]), \" |\"])\n",
    "\n",
    "        # print(\"Scoreself.state\") # TODO\n",
    "        # print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, num_actions):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def act(self, x):\n",
    "        return self(x).argmax()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greg's code\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, cap):\n",
    "        self.capacity = cap\n",
    "        self.data = []\n",
    "\n",
    "    def push(self, state, action, reward, nstate, term):\n",
    "        data = (state, action, reward, nstate, term)\n",
    "        if len(self.data) < self.capacity:\n",
    "            self.data.append(data)\n",
    "        else:\n",
    "            idx = random.randint(0, self.capacity - 1)\n",
    "            self.data[idx] = (data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.data, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # def view_last(self):\n",
    "    #     last = self.data[-1]\n",
    "    #     print(\"state:\", last[0], \"num pieces:\", np.sum(last[0]))\n",
    "    #     print(\"action:\", last[1])\n",
    "    #     print(\"reward:\", last[2])  \n",
    "    #     print(\"nstate:\", last[3], \"num pieces:\", np.sum(last[3]))  \n",
    "    #     print(\"term:\", last[4])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, lr=5e-4, episodes=2, batch_size=128, gamma=0.99, tau=0.005, capacity=10000, eps_start=0.9, eps_end=0.5, eps_rate=2000):\n",
    "    '''\n",
    "    Parameters:\n",
    "    env - the environment to train in\n",
    "    lr (float) - learning rate for policy updates\n",
    "    episodes (int) - the number of episodes to unroll in the environment during training\n",
    "    batch_size(int) - the size of each batch to use for policy updates\n",
    "    gamma (float) - the discount factor\n",
    "    tau (float) - size of updates to the target network\n",
    "    eps_start (float) - the initial probability of taking random actions\n",
    "    eps_end (float) - the final probability of taking random actions\n",
    "    eps_rate (float) - the rate of exponential decay for random action probability (higher is slower)\n",
    "    '''\n",
    "\n",
    "    # First we set up the policy and target networks\n",
    "    policy = DQN(env.state_dim, env.action_dim)\n",
    "    target = DQN(env.state_dim, env.action_dim)\n",
    "    \n",
    "    # We synchronize their parameters to start with\n",
    "    target.load_state_dict(policy.state_dict())\n",
    "\n",
    "    # Set up a dataset, optimizer, and loss function\n",
    "    memory = ReplayMemory(capacity)\n",
    "    opt = optim.Adam(policy.parameters(), lr=lr)\n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    # global_step will be used to randomize the policy early on\n",
    "    global_step = 0\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # The lengths of all of the trajectories \n",
    "    counts = []\n",
    "\n",
    "    # The rewards at the end of trajectories\n",
    "    rewards = []\n",
    "\n",
    "    for i in range(episodes):\n",
    "        print(\"EPISODE\", i)\n",
    "\n",
    "        sstate = env.reset()\n",
    "        print(sstate)\n",
    "        assert(np.sum(env.state) == 48)\n",
    "\n",
    "        count = 0\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            # Decide whether to take a random action or sample an action from the Q network\n",
    "            if rng.random() < eps_end + (eps_end - eps_start) * np.exp(-global_step / eps_rate):\n",
    "                action = rng.integers(0, env.action_dim)\n",
    "            else:\n",
    "                action = policy(torch.tensor(sstate, dtype=torch.float)).argmax()\n",
    "            \n",
    "            global_step += 1\n",
    "\n",
    "            # print(\"global_step =\",global_step)\n",
    "            # Take a step and store the results\n",
    "            sstate, istate, estate, reward, term, trunc, a, rand_a = env.step(action)\n",
    "\n",
    "            # Make sure that the next state has all the pieces\n",
    "            # assert(np.sum(nstate) == 48)\n",
    "\n",
    "            memory.push(sstate, action, reward, estate, term)\n",
    "            sstate = estate\n",
    "\n",
    "            print(\"global step:\", global_step)\n",
    "            print(\"count:\", count)\n",
    "            print(\"start state:\", sstate, \"(sum =\", np.sum(sstate), \")\")\n",
    "            print(\"learner action:\", a)\n",
    "            print(\"intermediate state:\", istate, \"(sum =\", np.sum(istate), \")\")\n",
    "            print(\"opponent action:\", rand_a)\n",
    "            print(\"end state:\", estate, \"(sum =\", np.sum(estate), \")\")\n",
    "            print(\"step reward:\", reward)\n",
    "            \n",
    "            \n",
    "            episode_reward += reward\n",
    "            print(\"episode reward:\", episode_reward)\n",
    "            print(\"\\n\")\n",
    "            count += 1\n",
    "\n",
    "\n",
    "        \n",
    "            # Update the policy network\n",
    "            if len(memory) >= batch_size:\n",
    "                batch = memory.sample(batch_size)\n",
    "                st_batch, act_batch, r_batch, nst_batch, t_batch = zip(*batch)\n",
    "                st_batch = torch.tensor(np.array(st_batch)).float()\n",
    "                act_batch = torch.tensor(np.array(act_batch)).unsqueeze(dim=1)\n",
    "                r_batch = torch.tensor(np.array(r_batch)).float()\n",
    "                nst_batch = torch.tensor(np.array(nst_batch)).float()\n",
    "                t_batch = torch.tensor(np.array(t_batch))\n",
    "\n",
    "                # pred_vals is the predicted Q value of the sampled\n",
    "                # state-action pairs from the dataset\n",
    "\n",
    "                # print(\"getting pred_vals...\")\n",
    "                pred_vals = policy(st_batch).gather(1, act_batch).squeeze()\n",
    "\n",
    "                # pred_next_vals is the predicted value of the sampled next\n",
    "                # states. This is where we use the trick of setting the value\n",
    "                # of terminal states to zero.\n",
    "                # print(\"getting pred_next_vals...\")\n",
    "                pred_next_vals = target(nst_batch).max(dim=1).values\n",
    "                pred_next_vals[t_batch] = 0\n",
    "\n",
    "                # expected_q is the right side of our loss from above.\n",
    "                expected_q = r_batch + gamma * pred_next_vals\n",
    "\n",
    "                # This part is just like what we've seen before.\n",
    "                loss_val = loss(pred_vals, expected_q)\n",
    "                opt.zero_grad()\n",
    "                loss_val.backward()\n",
    "                opt.step()\n",
    "\n",
    "            # Now we update the target network. This works by iterating over\n",
    "            # the parameters in the network and setting each to a weighted\n",
    "            # average of its current value and the corresponding parameter in\n",
    "            # the policy network.\n",
    "            p_state_dict = policy.state_dict()\n",
    "            t_state_dict = target.state_dict()\n",
    "            for key in p_state_dict:\n",
    "                t_state_dict[key] = p_state_dict[key] * tau + t_state_dict[key] * (1 - tau)\n",
    "            target.load_state_dict(t_state_dict)\n",
    "\n",
    "            # Finally, if the environment indicated that this is the end of\n",
    "            # a trajectory then we break out of the loop\n",
    "            if term or trunc:\n",
    "                counts.append(count)\n",
    "                rewards.append(episode_reward)\n",
    "                break\n",
    "\n",
    "        # if (i+1) % 5 == 0:\n",
    "        #     print(\"Episode:\", i, \" traj length:\", count)\n",
    "\n",
    "    return policy, counts, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 0\n",
      "[4 4 4 4 4 4 0 4 4 4 4 4 4 0]\n",
      "global step: 1\n",
      "count: 0\n",
      "start state: [4 4 4 4 4 0 1 5 0 6 5 5 5 1] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [4 4 4 4 4 0 1 5 0 6 5 5 5 1] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [4 4 4 4 4 0 1 5 0 6 5 5 5 1] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 2\n",
      "count: 1\n",
      "start state: [5 5 5 5 4 0 1 5 0 6 5 5 0 2] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [5 5 5 5 4 0 1 5 0 6 5 5 0 2] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [5 5 5 5 4 0 1 5 0 6 5 5 0 2] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 3\n",
      "count: 2\n",
      "start state: [5 5 5 5 4 0 1 0 1 7 6 6 1 2] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [5 5 5 5 4 0 1 0 1 7 6 6 1 2] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [5 5 5 5 4 0 1 0 1 7 6 6 1 2] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 4\n",
      "count: 3\n",
      "start state: [5 5 5 5 4 0 1 0 0 8 6 6 1 2] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [5 5 5 5 4 0 1 0 0 8 6 6 1 2] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [5 5 5 5 4 0 1 0 0 8 6 6 1 2] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 5\n",
      "count: 4\n",
      "start state: [5 5 5 5 4 0 1 0 0 8 6 6 0 3] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [5 5 5 5 4 0 1 0 0 8 6 6 0 3] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [5 5 5 5 4 0 1 0 0 8 6 6 0 3] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 6\n",
      "count: 5\n",
      "start state: [6 6 6 6 4 0 1 0 0 0 7 7 1 4] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [6 6 6 6 4 0 1 0 0 0 7 7 1 4] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [6 6 6 6 4 0 1 0 0 0 7 7 1 4] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 7\n",
      "count: 6\n",
      "start state: [7 7 7 7 5 0 1 0 0 0 7 0 2 5] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [7 7 7 7 5 0 1 0 0 0 7 0 2 5] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [7 7 7 7 5 0 1 0 0 0 7 0 2 5] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 8\n",
      "count: 7\n",
      "start state: [7 7 7 7 5 0 1 0 0 0 7 0 2 5] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [7 7 7 7 5 0 1 0 0 0 7 0 2 5] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [7 7 7 7 5 0 1 0 0 0 7 0 2 5] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 9\n",
      "count: 8\n",
      "start state: [8 8 8 8 5 0 1 0 0 0 0 1 3 6] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [8 8 8 8 5 0 1 0 0 0 0 1 3 6] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [8 8 8 8 5 0 1 0 0 0 0 1 3 6] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 10\n",
      "count: 9\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 11\n",
      "count: 10\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 12\n",
      "count: 11\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 13\n",
      "count: 12\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 14\n",
      "count: 13\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "learner action: 5\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 15\n",
      "count: 14\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 16\n",
      "count: 15\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 17\n",
      "count: 16\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 18\n",
      "count: 17\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 1 0 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 19\n",
      "count: 18\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 20\n",
      "count: 19\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 21\n",
      "count: 20\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 22\n",
      "count: 21\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 23\n",
      "count: 22\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 24\n",
      "count: 23\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 25\n",
      "count: 24\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 26\n",
      "count: 25\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 0 1 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 27\n",
      "count: 26\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 0 0 8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 0 0 8] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 0 0 8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 28\n",
      "count: 27\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 0 0 8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 0 0 8] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 0 0 8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 29\n",
      "count: 28\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 0 0 8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 0 0 8] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 0 0 8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 30\n",
      "count: 29\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 0 0 8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 0 0 8] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 0 0 8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 31\n",
      "count: 30\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 0 0 8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 0 0 8] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 0 0 8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "EPISODE 1\n",
      "[9 9 8 8 5 0 1 0 0 0 0 0 0 8]\n",
      "global step: 32\n",
      "count: 0\n",
      "start state: [9 9 8 8 5 0 1 0 0 0 0 0 0 8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 5 0 1 0 0 0 0 0 0 8] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [9 9 8 8 5 0 1 0 0 0 0 0 0 8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 33\n",
      "count: 1\n",
      "start state: [9 9 8 8 0 1 2 1 0 2 0 0 0 8] (sum = 48 )\n",
      "learner action: 4\n",
      "intermediate state: [9 9 8 8 0 1 2 1 0 2 0 0 0 8] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [9 9 8 8 0 1 2 1 0 2 0 0 0 8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 34\n",
      "count: 2\n",
      "start state: [9 9 8 8 0 0 3 1 0 2 0 0 0 8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 1 0 2 0 0 0 8] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [9 9 8 8 0 0 3 1 0 2 0 0 0 8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 35\n",
      "count: 3\n",
      "start state: [9 9 8 8 0 0 3 0 1 2 0 0 0 8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 1 2 0 0 0 8] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [9 9 8 8 0 0 3 0 1 2 0 0 0 8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 36\n",
      "count: 4\n",
      "start state: [9 9 8 8 0 0 3 0 1 0 1 1 0 8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 1 0 1 1 0 8] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [9 9 8 8 0 0 3 0 1 0 1 1 0 8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 37\n",
      "count: 5\n",
      "start state: [9 9 8 8 0 0 3 0 1 0 1 0 1 8] (sum = 48 )\n",
      "learner action: 5\n",
      "intermediate state: [9 9 8 8 0 0 3 0 1 0 1 0 1 8] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [9 9 8 8 0 0 3 0 1 0 1 0 1 8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 38\n",
      "count: 6\n",
      "start state: [9 9 8 8 0 0 3 0 0 1 1 0 1 8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 0 1 1 0 1 8] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [9 9 8 8 0 0 3 0 0 1 1 0 1 8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 39\n",
      "count: 7\n",
      "start state: [9 9 8 8 0 0 3 0 0 1 1 0 1 8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 0 1 1 0 1 8] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [9 9 8 8 0 0 3 0 0 1 1 0 1 8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 40\n",
      "count: 8\n",
      "start state: [9 9 8 8 0 0 3 0 0 1 1 0 1 8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 0 1 1 0 1 8] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [9 9 8 8 0 0 3 0 0 1 1 0 1 8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 41\n",
      "count: 9\n",
      "start state: [9 9 8 8 0 0 3 0 0 1 1 0 1 8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 0 1 1 0 1 8] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [9 9 8 8 0 0 3 0 0 1 1 0 1 8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 42\n",
      "count: 10\n",
      "start state: [9 9 8 8 0 0 3 0 0 0 2 0 1 8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 0 0 2 0 1 8] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [9 9 8 8 0 0 3 0 0 0 2 0 1 8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 43\n",
      "count: 11\n",
      "start state: [9 9 8 8 0 0 3 0 0 0 2 0 1 8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 0 0 2 0 1 8] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [9 9 8 8 0 0 3 0 0 0 2 0 1 8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 44\n",
      "count: 12\n",
      "start state: [9 9 8 8 0 0 3 0 0 0 2 0 0 9] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 0 0 2 0 0 9] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [9 9 8 8 0 0 3 0 0 0 2 0 0 9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 45\n",
      "count: 13\n",
      "start state: [9 9 8 8 0 0 3 0 0 0 2 0 0 9] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 0 0 2 0 0 9] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [9 9 8 8 0 0 3 0 0 0 2 0 0 9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 46\n",
      "count: 14\n",
      "start state: [9 9 8 8 0 0 3 0 0 0 2 0 0 9] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 0 0 2 0 0 9] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [9 9 8 8 0 0 3 0 0 0 2 0 0 9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 47\n",
      "count: 15\n",
      "start state: [9 9 8 8 0 0 3 0 0 0 2 0 0 9] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 0 0 2 0 0 9] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [9 9 8 8 0 0 3 0 0 0 2 0 0 9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 48\n",
      "count: 16\n",
      "start state: [9 9 8 8 0 0 3 0 0 0 0 1 1 9] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 0 0 0 1 1 9] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [9 9 8 8 0 0 3 0 0 0 0 1 1 9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 49\n",
      "count: 17\n",
      "start state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 50\n",
      "count: 18\n",
      "start state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 51\n",
      "count: 19\n",
      "start state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 52\n",
      "count: 20\n",
      "start state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 53\n",
      "count: 21\n",
      "start state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 54\n",
      "count: 22\n",
      "start state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 55\n",
      "count: 23\n",
      "start state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 56\n",
      "count: 24\n",
      "start state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [9 9 8 8 0 0 3 0 0 0 0 0 2 9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 57\n",
      "count: 25\n",
      "start state: [10  9  8  8  0  0  3  0  0  0  0  0  0 10] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [10  9  8  8  0  0  3  0  0  0  0  0  0 10] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [10  9  8  8  0  0  3  0  0  0  0  0  0 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 58\n",
      "count: 26\n",
      "start state: [10  9  8  8  0  0  3  0  0  0  0  0  0 10] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [10  9  8  8  0  0  3  0  0  0  0  0  0 10] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [10  9  8  8  0  0  3  0  0  0  0  0  0 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 59\n",
      "count: 27\n",
      "start state: [10  9  0  9  1  1  4  0  2  1  1  0  0 10] (sum = 48 )\n",
      "learner action: 2\n",
      "intermediate state: [10  9  0  9  1  1  4  0  2  1  1  0  0 10] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [10  9  0  9  1  1  4  0  2  1  1  0  0 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 60\n",
      "count: 28\n",
      "start state: [ 0 10  1 10  2  2  5  1  3  2  2  0  0 10] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 10  1 10  2  2  5  1  3  2  2  0  0 10] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0 10  1 10  2  2  5  1  3  2  2  0  0 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 61\n",
      "count: 29\n",
      "start state: [ 0 10  1 10  2  2  5  1  0  3  3  1  0 10] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 10  1 10  2  2  5  1  0  3  3  1  0 10] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0 10  1 10  2  2  5  1  0  3  3  1  0 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 62\n",
      "count: 30\n",
      "start state: [ 0 10  1 10  2  2  5  1  0  3  3  1  0 10] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 10  1 10  2  2  5  1  0  3  3  1  0 10] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0 10  1 10  2  2  5  1  0  3  3  1  0 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "EPISODE 2\n",
      "[ 0 10  1 10  2  2  5  1  0  3  3  1  0 10]\n",
      "global step: 63\n",
      "count: 0\n",
      "start state: [ 0 10  1 10  2  2  5  1  0  3  0  2  1 11] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 10  1 10  2  2  5  1  0  3  0  2  1 11] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0 10  1 10  2  2  5  1  0  3  0  2  1 11] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 64\n",
      "count: 1\n",
      "start state: [ 0 10  1 10  2  2  5  0  1  3  0  2  1 11] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 10  1 10  2  2  5  0  1  3  0  2  1 11] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0 10  1 10  2  2  5  0  1  3  0  2  1 11] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 65\n",
      "count: 2\n",
      "start state: [ 0 10  1 10  2  2  5  0  1  0  1  3  2 11] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 10  1 10  2  2  5  0  1  0  1  3  2 11] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0 10  1 10  2  2  5  0  1  0  1  3  2 11] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 66\n",
      "count: 3\n",
      "start state: [ 0 10  1 10  2  2  5  0  1  0  1  3  2 11] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 10  1 10  2  2  5  0  1  0  1  3  2 11] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0 10  1 10  2  2  5  0  1  0  1  3  2 11] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 67\n",
      "count: 4\n",
      "start state: [ 1 10  1 10  2  2  5  0  1  0  1  0  3 12] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 1 10  1 10  2  2  5  0  1  0  1  0  3 12] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1 10  1 10  2  2  5  0  1  0  1  0  3 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 68\n",
      "count: 5\n",
      "start state: [ 1 12  1 10  2  2  5  0  1  0  1  0  0 13] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 1 12  1 10  2  2  5  0  1  0  1  0  0 13] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 1 12  1 10  2  2  5  0  1  0  1  0  0 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 69\n",
      "count: 6\n",
      "start state: [ 0 13  1 10  2  2  5  0  1  0  1  0  0 13] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  1  0  1  0  0 13] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0 13  1 10  2  2  5  0  1  0  1  0  0 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 70\n",
      "count: 7\n",
      "start state: [ 0 13  1 10  2  2  5  0  1  0  1  0  0 13] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  1  0  1  0  0 13] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0 13  1 10  2  2  5  0  1  0  1  0  0 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 71\n",
      "count: 8\n",
      "start state: [ 0 13  1 10  2  2  5  0  1  0  1  0  0 13] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  1  0  1  0  0 13] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0 13  1 10  2  2  5  0  1  0  1  0  0 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 72\n",
      "count: 9\n",
      "start state: [ 0 13  1 10  2  2  5  0  0  1  1  0  0 13] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  0  1  1  0  0 13] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0 13  1 10  2  2  5  0  0  1  1  0  0 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 73\n",
      "count: 10\n",
      "start state: [ 0 13  1 10  2  2  5  0  0  1  1  0  0 13] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  0  1  1  0  0 13] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0 13  1 10  2  2  5  0  0  1  1  0  0 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 74\n",
      "count: 11\n",
      "start state: [ 0 13  1 10  2  2  5  0  0  0  2  0  0 13] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  0  0  2  0  0 13] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0 13  1 10  2  2  5  0  0  0  2  0  0 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 75\n",
      "count: 12\n",
      "start state: [ 0 13  1 10  2  2  5  0  0  0  0  1  1 13] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  0  0  0  1  1 13] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0 13  1 10  2  2  5  0  0  0  0  1  1 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 76\n",
      "count: 13\n",
      "start state: [ 0 13  1 10  2  2  5  0  0  0  0  1  1 13] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  0  0  0  1  1 13] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0 13  1 10  2  2  5  0  0  0  0  1  1 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 77\n",
      "count: 14\n",
      "start state: [ 0 13  1 10  2  2  5  0  0  0  0  1  0 14] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  0  0  0  1  0 14] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0 13  1 10  2  2  5  0  0  0  0  1  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 78\n",
      "count: 15\n",
      "start state: [ 0 13  1 10  2  2  5  0  0  0  0  0  1 14] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  0  0  0  0  1 14] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0 13  1 10  2  2  5  0  0  0  0  0  1 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 79\n",
      "count: 16\n",
      "start state: [ 0 13  1 10  2  2  5  0  0  0  0  0  1 14] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  0  0  0  0  1 14] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0 13  1 10  2  2  5  0  0  0  0  0  1 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 80\n",
      "count: 17\n",
      "start state: [ 0 13  1 10  2  2  5  0  0  0  0  0  1 14] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  0  0  0  0  1 14] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0 13  1 10  2  2  5  0  0  0  0  0  1 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 81\n",
      "count: 18\n",
      "start state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 82\n",
      "count: 19\n",
      "start state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 83\n",
      "count: 20\n",
      "start state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 84\n",
      "count: 21\n",
      "start state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 85\n",
      "count: 22\n",
      "start state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 86\n",
      "count: 23\n",
      "start state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 87\n",
      "count: 24\n",
      "start state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 88\n",
      "count: 25\n",
      "start state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0 13  1 10  2  2  5  0  0  0  0  0  0 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 89\n",
      "count: 26\n",
      "start state: [ 1  1  2 11  3  3  6  1  0  2  1  1  1 15] (sum = 48 )\n",
      "learner action: 1\n",
      "intermediate state: [ 1  1  2 11  3  3  6  1  0  2  1  1  1 15] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  1  2 11  3  3  6  1  0  2  1  1  1 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 90\n",
      "count: 27\n",
      "start state: [ 1  1  2 11  3  0  7  2  1  2  1  0  2 15] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 1  1  2 11  3  0  7  2  1  2  1  0  2 15] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1  1  2 11  3  0  7  2  1  2  1  0  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 91\n",
      "count: 28\n",
      "start state: [ 1  1  2 11  3  0  7  0  2  3  1  0  2 15] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 1  1  2 11  3  0  7  0  2  3  1  0  2 15] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  1  2 11  3  0  7  0  2  3  1  0  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 92\n",
      "count: 29\n",
      "start state: [ 1  1  2 11  3  0  7  0  2  3  1  0  2 15] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 1  1  2 11  3  0  7  0  2  3  1  0  2 15] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  1  2 11  3  0  7  0  2  3  1  0  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 93\n",
      "count: 30\n",
      "start state: [ 1  1  2 11  3  0  7  0  2  3  1  0  2 15] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 1  1  2 11  3  0  7  0  2  3  1  0  2 15] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  1  2 11  3  0  7  0  2  3  1  0  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "EPISODE 3\n",
      "[ 1  1  2 11  3  0  7  0  2  3  1  0  2 15]\n",
      "global step: 94\n",
      "count: 0\n",
      "start state: [ 1  1  2 11  0  1  8  1  2  0  2  1  3 15] (sum = 48 )\n",
      "learner action: 4\n",
      "intermediate state: [ 1  1  2 11  0  1  8  1  2  0  2  1  3 15] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 1  1  2 11  0  1  8  1  2  0  2  1  3 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 95\n",
      "count: 1\n",
      "start state: [ 1  1  2 11  0  0  9  1  2  0  2  1  3 15] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 1  1  2 11  0  0  9  1  2  0  2  1  3 15] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 1  1  2 11  0  0  9  1  2  0  2  1  3 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 96\n",
      "count: 2\n",
      "start state: [ 2  2  2 11  0  0  9  1  2  0  2  1  0 16] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2  2  2 11  0  0  9  1  2  0  2  1  0 16] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  2  2 11  0  0  9  1  2  0  2  1  0 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 97\n",
      "count: 3\n",
      "start state: [ 0  3  3 11  0  0  9  1  2  0  2  1  0 16] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  3  3 11  0  0  9  1  2  0  2  1  0 16] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  3  3 11  0  0  9  1  2  0  2  1  0 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 98\n",
      "count: 4\n",
      "start state: [ 0  3  3 11  0  0  9  1  2  0  2  0  1 16] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  3  3 11  0  0  9  1  2  0  2  0  1 16] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  3  3 11  0  0  9  1  2  0  2  0  1 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 99\n",
      "count: 5\n",
      "start state: [ 0  3  3 11  0  0  9  0  3  0  2  0  1 16] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  3  3 11  0  0  9  0  3  0  2  0  1 16] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  3  3 11  0  0  9  0  3  0  2  0  1 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 100\n",
      "count: 6\n",
      "start state: [ 0  3  3 11  0  0  9  0  3  0  2  0  1 16] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  3  3 11  0  0  9  0  3  0  2  0  1 16] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  3  3 11  0  0  9  0  3  0  2  0  1 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 101\n",
      "count: 7\n",
      "start state: [ 0  3  3 11  0  0  9  0  3  0  2  0  1 16] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  3  3 11  0  0  9  0  3  0  2  0  1 16] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  3  3 11  0  0  9  0  3  0  2  0  1 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 102\n",
      "count: 8\n",
      "start state: [ 0  3  3 11  0  0  9  0  3  0  2  0  0 17] (sum = 48 )\n",
      "learner action: 4\n",
      "intermediate state: [ 0  3  3 11  0  0  9  0  3  0  2  0  0 17] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0  3  3 11  0  0  9  0  3  0  2  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 103\n",
      "count: 9\n",
      "start state: [ 0  3  3 11  0  0  9  0  0  1  3  1  0 17] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  3  3 11  0  0  9  0  0  1  3  1  0 17] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  3  3 11  0  0  9  0  0  1  3  1  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 104\n",
      "count: 10\n",
      "start state: [ 0  3  3 11  0  0  9  0  0  1  3  1  0 17] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  3  3 11  0  0  9  0  0  1  3  1  0 17] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  3  3 11  0  0  9  0  0  1  3  1  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 105\n",
      "count: 11\n",
      "start state: [ 0  3  3 11  0  0  9  0  0  1  0  2  1 18] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  3  3 11  0  0  9  0  0  1  0  2  1 18] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  3  3 11  0  0  9  0  0  1  0  2  1 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 106\n",
      "count: 12\n",
      "start state: [ 0  3  3 11  0  0  9  0  0  1  0  2  1 18] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  3  3 11  0  0  9  0  0  1  0  2  1 18] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  3  3 11  0  0  9  0  0  1  0  2  1 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 107\n",
      "count: 13\n",
      "start state: [ 0  3  3 11  0  0  9  0  0  1  0  0  2 19] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  3  3 11  0  0  9  0  0  1  0  0  2 19] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  3  3 11  0  0  9  0  0  1  0  0  2 19] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 108\n",
      "count: 14\n",
      "start state: [ 0  3  3 11  0  0  9  0  0  1  0  0  2 19] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  3  3 11  0  0  9  0  0  1  0  0  2 19] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  3  3 11  0  0  9  0  0  1  0  0  2 19] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 109\n",
      "count: 15\n",
      "start state: [ 0  3  3 11  0  0  9  0  0  1  0  0  2 19] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  3  3 11  0  0  9  0  0  1  0  0  2 19] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  3  3 11  0  0  9  0  0  1  0  0  2 19] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 110\n",
      "count: 16\n",
      "start state: [ 0  3  3 11  0  0  9  0  0  1  0  0  2 19] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  3  3 11  0  0  9  0  0  1  0  0  2 19] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  3  3 11  0  0  9  0  0  1  0  0  2 19] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 111\n",
      "count: 17\n",
      "start state: [ 1  3  3 11  0  0  9  0  0  1  0  0  0 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 1  3  3 11  0  0  9  0  0  1  0  0  0 20] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 1  3  3 11  0  0  9  0  0  1  0  0  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 112\n",
      "count: 18\n",
      "start state: [ 0  4  3 11  0  0  9  0  0  0  1  0  0 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  3 11  0  0  9  0  0  0  1  0  0 20] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  4  3 11  0  0  9  0  0  0  1  0  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 113\n",
      "count: 19\n",
      "start state: [ 0  4  3 11  0  0  9  0  0  0  0  1  0 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  3 11  0  0  9  0  0  0  0  1  0 20] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  4  3 11  0  0  9  0  0  0  0  1  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 114\n",
      "count: 20\n",
      "start state: [ 0  4  3 11  0  0  9  0  0  0  0  1  0 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  3 11  0  0  9  0  0  0  0  1  0 20] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  4  3 11  0  0  9  0  0  0  0  1  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 115\n",
      "count: 21\n",
      "start state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 116\n",
      "count: 22\n",
      "start state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 117\n",
      "count: 23\n",
      "start state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 118\n",
      "count: 24\n",
      "start state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 119\n",
      "count: 25\n",
      "start state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 120\n",
      "count: 26\n",
      "start state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 121\n",
      "count: 27\n",
      "start state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 122\n",
      "count: 28\n",
      "start state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 123\n",
      "count: 29\n",
      "start state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  4  3 11  0  0  9  0  0  0  0  0  1 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 124\n",
      "count: 30\n",
      "start state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "EPISODE 4\n",
      "[ 0  4  3 11  0  0  9  0  0  0  0  0  0 21]\n",
      "global step: 125\n",
      "count: 0\n",
      "start state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 126\n",
      "count: 1\n",
      "start state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 127\n",
      "count: 2\n",
      "start state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: 5\n",
      "intermediate state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 128\n",
      "count: 3\n",
      "start state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 129\n",
      "count: 4\n",
      "start state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 130\n",
      "count: 5\n",
      "start state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  4  3 11  0  0  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 131\n",
      "count: 6\n",
      "start state: [ 0  4  0 12  1  1  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: 2\n",
      "intermediate state: [ 0  4  0 12  1  1  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  4  0 12  1  1  9  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 132\n",
      "count: 7\n",
      "start state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 133\n",
      "count: 8\n",
      "start state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 134\n",
      "count: 9\n",
      "start state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 135\n",
      "count: 10\n",
      "start state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 136\n",
      "count: 11\n",
      "start state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 137\n",
      "count: 12\n",
      "start state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 138\n",
      "count: 13\n",
      "start state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 139\n",
      "count: 14\n",
      "start state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 140\n",
      "count: 15\n",
      "start state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 141\n",
      "count: 16\n",
      "start state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 142\n",
      "count: 17\n",
      "start state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 143\n",
      "count: 18\n",
      "start state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  4  0 12  1  0 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 144\n",
      "count: 19\n",
      "start state: [ 0  0  1 13  2  1 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: 1\n",
      "intermediate state: [ 0  0  1 13  2  1 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  0  1 13  2  1 10  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 145\n",
      "count: 20\n",
      "start state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 146\n",
      "count: 21\n",
      "start state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 147\n",
      "count: 22\n",
      "start state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 148\n",
      "count: 23\n",
      "start state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 149\n",
      "count: 24\n",
      "start state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 150\n",
      "count: 25\n",
      "start state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 151\n",
      "count: 26\n",
      "start state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 152\n",
      "count: 27\n",
      "start state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 153\n",
      "count: 28\n",
      "start state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 154\n",
      "count: 29\n",
      "start state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 155\n",
      "count: 30\n",
      "start state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  0  1 13  2  0 11  0  0  0  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = GameEnv(30)\n",
    "policy, counts, rewards = train(env, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x3129c86d0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
