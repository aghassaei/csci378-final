{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineRenderer.figure_format = 'retina'\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import random\n",
    "import torch.utils.tensorboard as tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Environment\n",
    "\n",
    "The game environment `GameEnv` is formally given by \n",
    "\n",
    "**State space:** $14$ since there are $14$ holes on the board\n",
    "\n",
    "**Action space:** $6$ since there are $6$ holes a player can choose to redistribute\n",
    "\n",
    "**Reward function:** TODO\n",
    "\n",
    "\n",
    "The limit of the trajectories is 200 moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv:\n",
    "    def __init__(self, limit):\n",
    "        self.state = None\n",
    "        self.count = 0\n",
    "        self.limit = limit\n",
    "        self.state_dim = 14\n",
    "        self.action_dim = 6\n",
    "        self.rng = np.random.default_rng()\n",
    "     \n",
    "\n",
    "    def reset(self, state=np.array([4,4,4,4,4,4,0,4,4,4,4,4,4,0])):\n",
    "        self.state = state\n",
    "        self.count = 0\n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    def step(self, a):\n",
    "        '''Parameters:\n",
    "            a (int) : the action the learner will take'''\n",
    "\n",
    "        self.count += 1\n",
    "\n",
    "        # Keep track of the state before the learner moves\n",
    "        sstate = self.state.copy()\n",
    "\n",
    "        # Intermediate state, the state the learner will modify\n",
    "        istate = sstate.copy()\n",
    "\n",
    "        # Pieces the learner will move\n",
    "        pieces = istate[a]\n",
    "\n",
    "        # Set the state index to 0\n",
    "        istate[a] = 0\n",
    "\n",
    "        # First index to place pieces in \n",
    "        index = (a + 1) % 13\n",
    "        \n",
    "        # Distribute pieces\n",
    "        while pieces > 0:\n",
    "            istate[index] += 1\n",
    "            index = (index + 1) % 13\n",
    "            pieces -= 1\n",
    "        \n",
    "        # end state\n",
    "        estate = istate.copy()\n",
    "\n",
    "        # Random opponent\n",
    "        # Pick random action\n",
    "        rand_a = self.rng.integers(7, 13)\n",
    "        rand_pieces = estate[rand_a]\n",
    "        estate[rand_a] = 0\n",
    "\n",
    "        rand_index = (rand_a + 1) % 14\n",
    "\n",
    "        while rand_pieces > 0:\n",
    "            if rand_index != 6:\n",
    "                estate[rand_index] += 1\n",
    "                rand_pieces = rand_pieces - 1\n",
    "\n",
    "            rand_index = (rand_index + 1) % 14\n",
    "                \n",
    "\n",
    "        terminated = (estate[6] > 24) or (estate[13] > 24)\n",
    "        truncated = self.count > self.limit\n",
    "\n",
    "        # Reward every time learner is ahead\n",
    "        if estate[6] > estate[13]:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        \n",
    "        self.state = estate\n",
    "\n",
    "        return sstate, istate, self.state, reward, terminated, truncated, a, rand_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, num_actions):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def act(self, x):\n",
    "        return self(x).argmax()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greg's code\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, cap):\n",
    "        self.capacity = cap\n",
    "        self.data = []\n",
    "\n",
    "    def push(self, state, action, reward, nstate, term):\n",
    "        data = (state, action, reward, nstate, term)\n",
    "        if len(self.data) < self.capacity:\n",
    "            self.data.append(data)\n",
    "        else:\n",
    "            idx = random.randint(0, self.capacity - 1)\n",
    "            self.data[idx] = (data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.data, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # def view_last(self):\n",
    "    #     last = self.data[-1]\n",
    "    #     print(\"state:\", last[0], \"num pieces:\", np.sum(last[0]))\n",
    "    #     print(\"action:\", last[1])\n",
    "    #     print(\"reward:\", last[2])  \n",
    "    #     print(\"nstate:\", last[3], \"num pieces:\", np.sum(last[3]))  \n",
    "    #     print(\"term:\", last[4])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, lr=5e-4, episodes=2, batch_size=128, gamma=0.99, tau=0.005, capacity=10000, eps_start=0.9, eps_end=0.5, eps_rate=2000):\n",
    "    '''\n",
    "    Parameters:\n",
    "    env - the environment to train in\n",
    "    lr (float) - learning rate for policy updates\n",
    "    episodes (int) - the number of episodes to unroll in the environment during training\n",
    "    batch_size(int) - the size of each batch to use for policy updates\n",
    "    gamma (float) - the discount factor\n",
    "    tau (float) - size of updates to the target network\n",
    "    eps_start (float) - the initial probability of taking random actions\n",
    "    eps_end (float) - the final probability of taking random actions\n",
    "    eps_rate (float) - the rate of exponential decay for random action probability (higher is slower)\n",
    "    '''\n",
    "\n",
    "    # First we set up the policy and target networks\n",
    "    policy = DQN(env.state_dim, env.action_dim)\n",
    "    target = DQN(env.state_dim, env.action_dim)\n",
    "    \n",
    "    # We synchronize their parameters to start with\n",
    "    target.load_state_dict(policy.state_dict())\n",
    "\n",
    "    # Set up a dataset, optimizer, and loss function\n",
    "    memory = ReplayMemory(capacity)\n",
    "    opt = optim.Adam(policy.parameters(), lr=lr)\n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    # global_step will be used to randomize the policy early on\n",
    "    global_step = 0\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # The lengths of all of the trajectories \n",
    "    counts = []\n",
    "\n",
    "    # The rewards at the end of trajectories\n",
    "    rewards = []\n",
    "\n",
    "    for i in range(episodes):\n",
    "        print(\"EPISODE\", i)\n",
    "\n",
    "        sstate = env.reset()\n",
    "        print(sstate)\n",
    "        assert(np.sum(env.state) == 48)\n",
    "\n",
    "        count = 0\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            # Decide whether to take a random action or sample an action from the Q network\n",
    "            if rng.random() < eps_end + (eps_end - eps_start) * np.exp(-global_step / eps_rate):\n",
    "                action = rng.integers(0, env.action_dim)\n",
    "            else:\n",
    "                action = policy(torch.tensor(sstate, dtype=torch.float)).argmax()\n",
    "            \n",
    "            global_step += 1\n",
    "\n",
    "            # print(\"global_step =\",global_step)\n",
    "            # Take a step and store the results\n",
    "            sstate, istate, estate, reward, term, trunc, a, rand_a = env.step(action)\n",
    "\n",
    "            # Make sure that the next state has all the pieces\n",
    "            # assert(np.sum(nstate) == 48)\n",
    "\n",
    "            memory.push(sstate, action, reward, estate, term)\n",
    "            \n",
    "\n",
    "            print(\"global step:\", global_step)\n",
    "            print(\"count:\", count)\n",
    "            print(\"start state:\", sstate, \"(sum =\", np.sum(sstate), \")\")\n",
    "            print(\"learner action:\", a)\n",
    "            print(\"intermediate state:\", istate, \"(sum =\", np.sum(istate), \")\")\n",
    "            print(\"opponent action:\", rand_a)\n",
    "            print(\"end state:\", estate, \"(sum =\", np.sum(estate), \")\")\n",
    "            print(\"step reward:\", reward)\n",
    "            \n",
    "            \n",
    "            episode_reward += reward\n",
    "            print(\"episode reward:\", episode_reward)\n",
    "            print(\"\\n\")\n",
    "            count += 1\n",
    "\n",
    "            sstate = estate\n",
    "        \n",
    "            # Update the policy network\n",
    "            if len(memory) >= batch_size:\n",
    "                batch = memory.sample(batch_size)\n",
    "                st_batch, act_batch, r_batch, nst_batch, t_batch = zip(*batch)\n",
    "                st_batch = torch.tensor(np.array(st_batch)).float()\n",
    "                act_batch = torch.tensor(np.array(act_batch)).unsqueeze(dim=1)\n",
    "                r_batch = torch.tensor(np.array(r_batch)).float()\n",
    "                nst_batch = torch.tensor(np.array(nst_batch)).float()\n",
    "                t_batch = torch.tensor(np.array(t_batch))\n",
    "\n",
    "                # pred_vals is the predicted Q value of the sampled\n",
    "                # state-action pairs from the dataset\n",
    "\n",
    "                # print(\"getting pred_vals...\")\n",
    "                pred_vals = policy(st_batch).gather(1, act_batch).squeeze()\n",
    "\n",
    "                # pred_next_vals is the predicted value of the sampled next\n",
    "                # states. This is where we use the trick of setting the value\n",
    "                # of terminal states to zero.\n",
    "                # print(\"getting pred_next_vals...\")\n",
    "                pred_next_vals = target(nst_batch).max(dim=1).values\n",
    "                pred_next_vals[t_batch] = 0\n",
    "\n",
    "                # expected_q is the right side of our loss from above.\n",
    "                expected_q = r_batch + gamma * pred_next_vals\n",
    "\n",
    "                # This part is just like what we've seen before.\n",
    "                loss_val = loss(pred_vals, expected_q)\n",
    "                opt.zero_grad()\n",
    "                loss_val.backward()\n",
    "                opt.step()\n",
    "\n",
    "            # Now we update the target network. This works by iterating over\n",
    "            # the parameters in the network and setting each to a weighted\n",
    "            # average of its current value and the corresponding parameter in\n",
    "            # the policy network.\n",
    "            p_state_dict = policy.state_dict()\n",
    "            t_state_dict = target.state_dict()\n",
    "            for key in p_state_dict:\n",
    "                t_state_dict[key] = p_state_dict[key] * tau + t_state_dict[key] * (1 - tau)\n",
    "            target.load_state_dict(t_state_dict)\n",
    "\n",
    "            # Finally, if the environment indicated that this is the end of\n",
    "            # a trajectory then we break out of the loop\n",
    "            if term or trunc:\n",
    "                counts.append(count)\n",
    "                rewards.append(episode_reward)\n",
    "                break\n",
    "\n",
    "        # if (i+1) % 5 == 0:\n",
    "        #     print(\"Episode:\", i, \" traj length:\", count)\n",
    "\n",
    "    return policy, counts, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 0\n",
      "[4 4 4 4 4 4 0 4 4 4 4 4 4 0]\n",
      "global step: 1\n",
      "count: 0\n",
      "start state: [4 4 4 4 4 4 0 4 4 4 4 4 4 0] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [0 5 5 5 5 4 0 4 4 4 4 4 4 0] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [1 5 5 5 5 4 0 4 4 4 0 5 5 1] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 2\n",
      "count: 1\n",
      "start state: [1 5 5 5 5 4 0 4 4 4 0 5 5 1] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [0 6 5 5 5 4 0 4 4 4 0 5 5 1] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [0 6 5 5 5 4 0 4 0 5 1 6 6 1] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 3\n",
      "count: 2\n",
      "start state: [0 6 5 5 5 4 0 4 0 5 1 6 6 1] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [0 6 5 5 5 4 0 4 0 5 1 6 6 1] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [0 6 5 5 5 4 0 4 0 5 1 6 6 1] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 4\n",
      "count: 3\n",
      "start state: [0 6 5 5 5 4 0 4 0 5 1 6 6 1] (sum = 48 )\n",
      "learner action: 1\n",
      "intermediate state: [0 0 6 6 6 5 1 5 0 5 1 6 6 1] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [1 1 7 7 7 5 1 5 0 5 1 6 0 2] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 5\n",
      "count: 4\n",
      "start state: [1 1 7 7 7 5 1 5 0 5 1 6 0 2] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [1 1 7 7 7 0 2 6 1 6 2 6 0 2] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [1 1 7 7 7 0 2 6 1 6 0 7 1 2] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 6\n",
      "count: 5\n",
      "start state: [1 1 7 7 7 0 2 6 1 6 0 7 1 2] (sum = 48 )\n",
      "learner action: 4\n",
      "intermediate state: [1 1 7 7 0 1 3 7 2 7 1 8 1 2] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [1 1 7 7 0 1 3 7 2 7 1 8 0 3] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 7\n",
      "count: 6\n",
      "start state: [1 1 7 7 0 1 3 7 2 7 1 8 0 3] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [1 1 7 7 0 0 4 7 2 7 1 8 0 3] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [2 1 7 7 0 0 4 0 3 8 2 9 1 4] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 8\n",
      "count: 7\n",
      "start state: [2 1 7 7 0 0 4 0 3 8 2 9 1 4] (sum = 48 )\n",
      "learner action: 5\n",
      "intermediate state: [2 1 7 7 0 0 4 0 3 8 2 9 1 4] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [3 2 8 8 1 1 4 1 3 8 2 0 2 5] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 9\n",
      "count: 8\n",
      "start state: [3 2 8 8 1 1 4 1 3 8 2 0 2 5] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [0 3 9 9 1 1 4 1 3 8 2 0 2 5] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [0 3 9 9 1 1 4 1 3 8 0 1 3 5] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 10\n",
      "count: 9\n",
      "start state: [0 3 9 9 1 1 4 1 3 8 0 1 3 5] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [0 3 9 0 2 2 5 2 4 9 1 2 4 5] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 1  4 10  1  3  2  5  2  4  0  2  3  5  6] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 11\n",
      "count: 10\n",
      "start state: [ 1  4 10  1  3  2  5  2  4  0  2  3  5  6] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  4 10  0  4  2  5  2  4  0  2  3  5  6] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  4 10  0  4  2  5  2  0  1  3  4  6  6] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 12\n",
      "count: 11\n",
      "start state: [ 1  4 10  0  4  2  5  2  0  1  3  4  6  6] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  4 10  0  4  2  5  2  0  1  3  4  6  6] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  5 10  0  4  2  5  2  0  1  3  0  7  7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 13\n",
      "count: 12\n",
      "start state: [ 2  5 10  0  4  2  5  2  0  1  3  0  7  7] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 10  0  4  2  5  2  0  1  3  0  7  7] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  5 10  0  4  2  5  0  1  2  3  0  7  7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 14\n",
      "count: 13\n",
      "start state: [ 2  5 10  0  4  2  5  0  1  2  3  0  7  7] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 10  0  4  2  5  0  1  2  3  0  7  7] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  5 10  0  4  2  5  0  1  2  0  1  8  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 15\n",
      "count: 14\n",
      "start state: [ 2  5 10  0  4  2  5  0  1  2  0  1  8  8] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 10  0  4  2  5  0  1  2  0  1  8  8] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  5 10  0  4  2  5  0  1  2  0  1  8  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 16\n",
      "count: 15\n",
      "start state: [ 2  5 10  0  4  2  5  0  1  2  0  1  8  8] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 10  0  4  2  5  0  1  2  0  1  8  8] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  5 10  0  4  2  5  0  0  3  0  1  8  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 17\n",
      "count: 16\n",
      "start state: [ 2  5 10  0  4  2  5  0  0  3  0  1  8  8] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 10  0  4  2  5  0  0  3  0  1  8  8] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  5 10  0  4  2  5  0  0  3  0  1  8  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 18\n",
      "count: 17\n",
      "start state: [ 2  5 10  0  4  2  5  0  0  3  0  1  8  8] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 10  0  4  2  5  0  0  3  0  1  8  8] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  5 10  0  4  2  5  0  0  3  0  0  9  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 19\n",
      "count: 18\n",
      "start state: [ 2  5 10  0  4  2  5  0  0  3  0  0  9  8] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 10  0  4  2  5  0  0  3  0  0  9  8] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  5 10  0  4  2  5  0  0  3  0  0  9  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 20\n",
      "count: 19\n",
      "start state: [ 2  5 10  0  4  2  5  0  0  3  0  0  9  8] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 10  0  4  2  5  0  0  3  0  0  9  8] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  5 10  0  4  2  5  0  0  3  0  0  9  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 21\n",
      "count: 20\n",
      "start state: [ 2  5 10  0  4  2  5  0  0  3  0  0  9  8] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 10  0  4  2  5  0  0  3  0  0  9  8] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  5 10  0  4  2  5  0  0  3  0  0  9  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 22\n",
      "count: 21\n",
      "start state: [ 2  5 10  0  4  2  5  0  0  3  0  0  9  8] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 10  0  4  2  5  0  0  3  0  0  9  8] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  5 10  0  4  2  5  0  0  3  0  0  9  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 23\n",
      "count: 22\n",
      "start state: [ 2  5 10  0  4  2  5  0  0  3  0  0  9  8] (sum = 48 )\n",
      "learner action: 1\n",
      "intermediate state: [ 2  0 11  1  5  3  6  0  0  3  0  0  9  8] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  0 11  1  5  3  6  0  0  3  0  0  9  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 24\n",
      "count: 23\n",
      "start state: [ 2  0 11  1  5  3  6  0  0  3  0  0  9  8] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  0 11  0  6  3  6  0  0  3  0  0  9  8] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  0 11  0  6  3  6  0  0  3  0  0  9  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 25\n",
      "count: 24\n",
      "start state: [ 2  0 11  0  6  3  6  0  0  3  0  0  9  8] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  0 11  0  6  3  6  0  0  3  0  0  9  8] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 2  0 11  0  6  3  6  0  0  0  1  1 10  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 26\n",
      "count: 25\n",
      "start state: [ 2  0 11  0  6  3  6  0  0  0  1  1 10  8] (sum = 48 )\n",
      "learner action: 3\n",
      "intermediate state: [ 2  0 11  0  6  3  6  0  0  0  1  1 10  8] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  0 11  0  6  3  6  0  0  0  1  1 10  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 27\n",
      "count: 26\n",
      "start state: [ 2  0 11  0  6  3  6  0  0  0  1  1 10  8] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  0 11  0  6  3  6  0  0  0  1  1 10  8] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  0 11  0  6  3  6  0  0  0  1  1 10  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 28\n",
      "count: 27\n",
      "start state: [ 2  0 11  0  6  3  6  0  0  0  1  1 10  8] (sum = 48 )\n",
      "learner action: 4\n",
      "intermediate state: [ 2  0 11  0  0  4  7  1  1  1  2  1 10  8] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  0 11  0  0  4  7  0  2  1  2  1 10  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 29\n",
      "count: 28\n",
      "start state: [ 2  0 11  0  0  4  7  0  2  1  2  1 10  8] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  0 11  0  0  4  7  0  2  1  2  1 10  8] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  0 11  0  0  4  7  0  0  2  3  1 10  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 30\n",
      "count: 29\n",
      "start state: [ 2  0 11  0  0  4  7  0  0  2  3  1 10  8] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  0 11  0  0  4  7  0  0  2  3  1 10  8] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  0 11  0  0  4  7  0  0  2  3  1 10  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 31\n",
      "count: 30\n",
      "start state: [ 2  0 11  0  0  4  7  0  0  2  3  1 10  8] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  0 11  0  0  4  7  0  0  2  3  1 10  8] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 3  1 12  1  1  5  7  1  1  3  3  1  0  9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 32\n",
      "count: 31\n",
      "start state: [ 3  1 12  1  1  5  7  1  1  3  3  1  0  9] (sum = 48 )\n",
      "learner action: 2\n",
      "intermediate state: [4 2 0 2 2 6 8 2 2 4 4 2 1 9] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 4  2  0  2  2  6  8  2  2  4  4  0  2 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 33\n",
      "count: 32\n",
      "start state: [ 4  2  0  2  2  6  8  2  2  4  4  0  2 10] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 4  2  0  2  2  6  8  2  2  4  4  0  2 10] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 4  2  0  2  2  6  8  2  0  5  5  0  2 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 34\n",
      "count: 33\n",
      "start state: [ 4  2  0  2  2  6  8  2  0  5  5  0  2 10] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 4  0  1  3  2  6  8  2  0  5  5  0  2 10] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 5  0  1  3  2  6  8  2  0  0  6  1  3 11] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 35\n",
      "count: 34\n",
      "start state: [ 5  0  1  3  2  6  8  2  0  0  6  1  3 11] (sum = 48 )\n",
      "learner action: 0\n",
      "intermediate state: [ 0  1  2  4  3  7  8  2  0  0  6  1  3 11] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 1  2  2  4  3  7  8  2  0  0  6  1  0 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 36\n",
      "count: 35\n",
      "start state: [ 1  2  2  4  3  7  8  2  0  0  6  1  0 12] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0  3  5  3  7  8  2  0  0  6  1  0 12] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  0  3  5  3  7  8  2  0  0  6  1  0 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 37\n",
      "count: 36\n",
      "start state: [ 1  0  3  5  3  7  8  2  0  0  6  1  0 12] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0  3  5  3  7  8  2  0  0  6  1  0 12] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  1  4  5  3  7  8  2  0  0  0  2  1 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 38\n",
      "count: 37\n",
      "start state: [ 2  1  4  5  3  7  8  2  0  0  0  2  1 13] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  1  4  0  4  8  9  3  1  0  0  2  1 13] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 2  1  4  0  4  8  9  3  1  0  0  2  1 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 39\n",
      "count: 38\n",
      "start state: [ 2  1  4  0  4  8  9  3  1  0  0  2  1 13] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  1  0  1  5  9 10  3  1  0  0  2  1 13] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  1  0  1  5  9 10  3  0  1  0  2  1 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 40\n",
      "count: 39\n",
      "start state: [ 2  1  0  1  5  9 10  3  0  1  0  2  1 13] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  1  0  1  5  9 10  3  0  1  0  2  1 13] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  1  0  1  5  9 10  0  1  2  1  2  1 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 41\n",
      "count: 40\n",
      "start state: [ 2  1  0  1  5  9 10  0  1  2  1  2  1 13] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  1  0  1  5  9 10  0  1  2  1  2  1 13] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  1  0  1  5  9 10  0  1  2  1  0  2 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 42\n",
      "count: 41\n",
      "start state: [ 2  1  0  1  5  9 10  0  1  2  1  0  2 14] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  1  0  1  5  9 10  0  1  2  1  0  2 14] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  1  0  1  5  9 10  0  0  3  1  0  2 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 43\n",
      "count: 42\n",
      "start state: [ 2  1  0  1  5  9 10  0  0  3  1  0  2 14] (sum = 48 )\n",
      "learner action: 3\n",
      "intermediate state: [ 2  1  0  0  6  9 10  0  0  3  1  0  2 14] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  1  0  0  6  9 10  0  0  3  0  1  2 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 44\n",
      "count: 43\n",
      "start state: [ 2  1  0  0  6  9 10  0  0  3  0  1  2 14] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  1  0  0  6  9 10  0  0  3  0  1  2 14] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  1  0  0  6  9 10  0  0  3  0  1  2 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 45\n",
      "count: 44\n",
      "start state: [ 2  1  0  0  6  9 10  0  0  3  0  1  2 14] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  1  0  0  6  9 10  0  0  3  0  1  2 14] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 3  1  0  0  6  9 10  0  0  3  0  1  0 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 46\n",
      "count: 45\n",
      "start state: [ 3  1  0  0  6  9 10  0  0  3  0  1  0 15] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 4  2  0  0  6  0 11  1  1  4  1  2  1 15] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 4  2  0  0  6  0 11  0  2  4  1  2  1 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 47\n",
      "count: 46\n",
      "start state: [ 4  2  0  0  6  0 11  0  2  4  1  2  1 15] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  3  1  1  7  0 11  0  2  4  1  2  1 15] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0  3  1  1  7  0 11  0  2  4  1  2  0 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 48\n",
      "count: 47\n",
      "start state: [ 0  3  1  1  7  0 11  0  2  4  1  2  0 16] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  3  1  0  8  0 11  0  2  4  1  2  0 16] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  3  1  0  8  0 11  0  2  4  0  3  0 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 49\n",
      "count: 48\n",
      "start state: [ 0  3  1  0  8  0 11  0  2  4  0  3  0 16] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  3  1  0  8  0 11  0  2  4  0  3  0 16] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  3  1  0  8  0 11  0  2  4  0  3  0 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 50\n",
      "count: 49\n",
      "start state: [ 0  3  1  0  8  0 11  0  2  4  0  3  0 16] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  3  1  0  8  0 11  0  2  4  0  3  0 16] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1  3  1  0  8  0 11  0  2  4  0  0  1 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 51\n",
      "count: 50\n",
      "start state: [ 1  3  1  0  8  0 11  0  2  4  0  0  1 17] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 1  3  0  1  8  0 11  0  2  4  0  0  1 17] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  3  0  1  8  0 11  0  0  5  1  0  1 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 52\n",
      "count: 51\n",
      "start state: [ 1  3  0  1  8  0 11  0  0  5  1  0  1 17] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  0  1  8  0 11  0  0  5  1  0  1 17] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 53\n",
      "count: 52\n",
      "start state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 54\n",
      "count: 53\n",
      "start state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 55\n",
      "count: 54\n",
      "start state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "learner action: 5\n",
      "intermediate state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 56\n",
      "count: 55\n",
      "start state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 57\n",
      "count: 56\n",
      "start state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 58\n",
      "count: 57\n",
      "start state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 59\n",
      "count: 58\n",
      "start state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 60\n",
      "count: 59\n",
      "start state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 61\n",
      "count: 60\n",
      "start state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 62\n",
      "count: 61\n",
      "start state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  4  0  1  8  0 11  0  0  5  1  0  0 18] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 1  4  0  1  8  0 11  0  0  0  2  1  1 19] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 63\n",
      "count: 62\n",
      "start state: [ 1  4  0  1  8  0 11  0  0  0  2  1  1 19] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 1  4  0  1  8  0 11  0  0  0  2  1  1 19] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 1  4  0  1  8  0 11  0  0  0  0  2  2 19] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 64\n",
      "count: 63\n",
      "start state: [ 1  4  0  1  8  0 11  0  0  0  0  2  2 19] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  5  0  1  8  0 11  0  0  0  0  2  2 19] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  5  0  1  8  0 11  0  0  0  0  2  2 19] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 65\n",
      "count: 64\n",
      "start state: [ 0  5  0  1  8  0 11  0  0  0  0  2  2 19] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  5  0  1  8  0 11  0  0  0  0  2  2 19] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 1  5  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 66\n",
      "count: 65\n",
      "start state: [ 1  5  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 67\n",
      "count: 66\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 68\n",
      "count: 67\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 69\n",
      "count: 68\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 70\n",
      "count: 69\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 71\n",
      "count: 70\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 72\n",
      "count: 71\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  2  0 20] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 73\n",
      "count: 72\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 74\n",
      "count: 73\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 75\n",
      "count: 74\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 76\n",
      "count: 75\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 77\n",
      "count: 76\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 78\n",
      "count: 77\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 79\n",
      "count: 78\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  0  1 21] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 80\n",
      "count: 79\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 81\n",
      "count: 80\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 82\n",
      "count: 81\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 83\n",
      "count: 82\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 84\n",
      "count: 83\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 85\n",
      "count: 84\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 86\n",
      "count: 85\n",
      "start state: [ 0  6  0  1  8  0 11  0  0  0  0  0  0 22] (sum = 48 )\n",
      "learner action: 1\n",
      "intermediate state: [ 0  0  1  2  9  1 12  1  0  0  0  0  0 22] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  0  1  2  9  1 12  1  0  0  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 87\n",
      "count: 86\n",
      "start state: [ 0  0  1  2  9  1 12  1  0  0  0  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  0  0  3  9  1 12  1  0  0  0  0  0 22] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0  0  0  3  9  1 12  1  0  0  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 88\n",
      "count: 87\n",
      "start state: [ 0  0  0  3  9  1 12  1  0  0  0  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  0  0  3  9  1 12  1  0  0  0  0  0 22] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  0  0  3  9  1 12  1  0  0  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 89\n",
      "count: 88\n",
      "start state: [ 0  0  0  3  9  1 12  1  0  0  0  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  0  0  3  9  1 12  1  0  0  0  0  0 22] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  0  0  3  9  1 12  1  0  0  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 90\n",
      "count: 89\n",
      "start state: [ 0  0  0  3  9  1 12  1  0  0  0  0  0 22] (sum = 48 )\n",
      "learner action: 2\n",
      "intermediate state: [ 0  0  0  3  9  1 12  1  0  0  0  0  0 22] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  0  0  3  9  1 12  1  0  0  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 91\n",
      "count: 90\n",
      "start state: [ 0  0  0  3  9  1 12  1  0  0  0  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  0  0  3  9  1 12  1  0  0  0  0  0 22] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  0  0  3  9  1 12  0  1  0  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 92\n",
      "count: 91\n",
      "start state: [ 0  0  0  3  9  1 12  0  1  0  0  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  0  0  3  9  1 12  0  1  0  0  0  0 22] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  0  0  3  9  1 12  0  1  0  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 93\n",
      "count: 92\n",
      "start state: [ 0  0  0  3  9  1 12  0  1  0  0  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  0  0  3  9  1 12  0  1  0  0  0  0 22] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  0  0  3  9  1 12  0  1  0  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 94\n",
      "count: 93\n",
      "start state: [ 0  0  0  3  9  1 12  0  1  0  0  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  0  0  3  9  1 12  0  1  0  0  0  0 22] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  0  0  3  9  1 12  0  0  1  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 95\n",
      "count: 94\n",
      "start state: [ 0  0  0  3  9  1 12  0  0  1  0  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  0  0  3  9  1 12  0  0  1  0  0  0 22] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  0  0  3  9  1 12  0  0  1  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 96\n",
      "count: 95\n",
      "start state: [ 0  0  0  3  9  1 12  0  0  1  0  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  0  0  3  9  1 12  0  0  1  0  0  0 22] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0  0  0  3  9  1 12  0  0  1  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 97\n",
      "count: 96\n",
      "start state: [ 0  0  0  3  9  1 12  0  0  1  0  0  0 22] (sum = 48 )\n",
      "learner action: 5\n",
      "intermediate state: [ 0  0  0  3  9  0 13  0  0  1  0  0  0 22] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  0  0  3  9  0 13  0  0  1  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 98\n",
      "count: 97\n",
      "start state: [ 0  0  0  3  9  0 13  0  0  1  0  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  0  0  3  9  0 13  0  0  1  0  0  0 22] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0  0  0  3  9  0 13  0  0  1  0  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 99\n",
      "count: 98\n",
      "start state: [ 0  0  0  3  9  0 13  0  0  1  0  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  0  0  3  9  0 13  0  0  1  0  0  0 22] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  0  0  3  9  0 13  0  0  0  1  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 100\n",
      "count: 99\n",
      "start state: [ 0  0  0  3  9  0 13  0  0  0  1  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  0  0  3  9  0 13  0  0  0  1  0  0 22] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  0  0  3  9  0 13  0  0  0  1  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 101\n",
      "count: 100\n",
      "start state: [ 0  0  0  3  9  0 13  0  0  0  1  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  0  0  3  9  0 13  0  0  0  1  0  0 22] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  0  0  3  9  0 13  0  0  0  1  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "EPISODE 1\n",
      "[4 4 4 4 4 4 0 4 4 4 4 4 4 0]\n",
      "global step: 102\n",
      "count: 0\n",
      "start state: [4 4 4 4 4 4 0 4 4 4 4 4 4 0] (sum = 48 )\n",
      "learner action: 4\n",
      "intermediate state: [4 4 4 4 0 5 1 5 5 4 4 4 4 0] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [4 4 4 4 0 5 1 0 6 5 5 5 5 0] (sum = 48 )\n",
      "step reward: 1\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 103\n",
      "count: 1\n",
      "start state: [4 4 4 4 0 5 1 0 6 5 5 5 5 0] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [0 5 5 5 1 5 1 0 6 5 5 5 5 0] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [1 6 6 6 1 5 1 0 6 5 5 5 0 1] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 104\n",
      "count: 2\n",
      "start state: [1 6 6 6 1 5 1 0 6 5 5 5 0 1] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [1 0 7 7 2 6 2 1 6 5 5 5 0 1] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [2 0 7 7 2 6 2 1 6 0 6 6 1 2] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 105\n",
      "count: 3\n",
      "start state: [2 0 7 7 2 6 2 1 6 0 6 6 1 2] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [2 0 7 7 2 6 2 1 6 0 6 6 1 2] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [3 0 7 7 2 6 2 1 0 1 7 7 2 3] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 106\n",
      "count: 4\n",
      "start state: [3 0 7 7 2 6 2 1 0 1 7 7 2 3] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [3 0 7 7 2 6 2 1 0 1 7 7 2 3] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [4 0 7 7 2 6 2 1 0 1 7 7 0 4] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 107\n",
      "count: 5\n",
      "start state: [4 0 7 7 2 6 2 1 0 1 7 7 0 4] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [4 0 7 7 2 6 2 1 0 1 7 7 0 4] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [4 0 7 7 2 6 2 0 1 1 7 7 0 4] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 108\n",
      "count: 6\n",
      "start state: [4 0 7 7 2 6 2 0 1 1 7 7 0 4] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [4 0 7 7 2 6 2 0 1 1 7 7 0 4] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [5 1 8 8 2 6 2 0 1 1 0 8 1 5] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 109\n",
      "count: 7\n",
      "start state: [5 1 8 8 2 6 2 0 1 1 0 8 1 5] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [5 1 8 8 2 0 3 1 2 2 1 9 1 5] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [6 2 9 9 3 1 3 2 2 2 1 0 2 6] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 110\n",
      "count: 8\n",
      "start state: [6 2 9 9 3 1 3 2 2 2 1 0 2 6] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [6 2 9 0 4 2 4 3 3 3 2 1 3 6] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [6 2 9 0 4 2 4 3 3 3 0 2 4 6] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 111\n",
      "count: 9\n",
      "start state: [6 2 9 0 4 2 4 3 3 3 0 2 4 6] (sum = 48 )\n",
      "learner action: 2\n",
      "intermediate state: [6 2 0 1 5 3 5 4 4 4 1 3 4 6] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [7 2 0 1 5 3 5 4 4 4 1 0 5 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 112\n",
      "count: 10\n",
      "start state: [7 2 0 1 5 3 5 4 4 4 1 0 5 7] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [0 3 1 2 6 4 6 5 4 4 1 0 5 7] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [0 3 1 2 6 4 6 5 4 4 1 0 5 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 113\n",
      "count: 11\n",
      "start state: [0 3 1 2 6 4 6 5 4 4 1 0 5 7] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [0 3 1 0 7 5 6 5 4 4 1 0 5 7] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [0 3 1 0 7 5 6 0 5 5 2 1 6 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 114\n",
      "count: 12\n",
      "start state: [0 3 1 0 7 5 6 0 5 5 2 1 6 7] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [0 3 0 1 7 5 6 0 5 5 2 1 6 7] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [0 3 0 1 7 5 6 0 0 6 3 2 7 8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 115\n",
      "count: 13\n",
      "start state: [0 3 0 1 7 5 6 0 0 6 3 2 7 8] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [0 3 0 1 7 5 6 0 0 6 3 2 7 8] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [1 4 0 1 7 5 6 0 0 0 4 3 8 9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 116\n",
      "count: 14\n",
      "start state: [1 4 0 1 7 5 6 0 0 0 4 3 8 9] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [1 4 0 1 7 5 6 0 0 0 4 3 8 9] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  4  0  1  7  5  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 117\n",
      "count: 15\n",
      "start state: [ 2  4  0  1  7  5  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  4  0  1  7  5  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  4  0  1  7  5  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 118\n",
      "count: 16\n",
      "start state: [ 2  4  0  1  7  5  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "learner action: 1\n",
      "intermediate state: [ 2  0  1  2  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  0  1  2  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 119\n",
      "count: 17\n",
      "start state: [ 2  0  1  2  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 120\n",
      "count: 18\n",
      "start state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 121\n",
      "count: 19\n",
      "start state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 122\n",
      "count: 20\n",
      "start state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 123\n",
      "count: 21\n",
      "start state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 124\n",
      "count: 22\n",
      "start state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 125\n",
      "count: 23\n",
      "start state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 126\n",
      "count: 24\n",
      "start state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 127\n",
      "count: 25\n",
      "start state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  0  0  3  8  6  6  0  0  0  4  0  9 10] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 3  0  0  3  8  6  6  0  0  0  0  1 10 11] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 128\n",
      "count: 26\n",
      "start state: [ 3  0  0  3  8  6  6  0  0  0  0  1 10 11] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 3  0  0  3  8  6  6  0  0  0  0  1 10 11] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 3  0  0  3  8  6  6  0  0  0  0  1 10 11] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 129\n",
      "count: 27\n",
      "start state: [ 3  0  0  3  8  6  6  0  0  0  0  1 10 11] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 3  0  0  3  8  6  6  0  0  0  0  1 10 11] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 4  1  1  4  9  7  6  1  1  1  0  1  0 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 130\n",
      "count: 28\n",
      "start state: [ 4  1  1  4  9  7  6  1  1  1  0  1  0 12] (sum = 48 )\n",
      "learner action: 1\n",
      "intermediate state: [ 4  0  2  4  9  7  6  1  1  1  0  1  0 12] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 4  0  2  4  9  7  6  1  0  2  0  1  0 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 131\n",
      "count: 29\n",
      "start state: [ 4  0  2  4  9  7  6  1  0  2  0  1  0 12] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 4  0  2  4  9  7  6  1  0  2  0  1  0 12] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 4  0  2  4  9  7  6  1  0  2  0  1  0 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 132\n",
      "count: 30\n",
      "start state: [ 4  0  2  4  9  7  6  1  0  2  0  1  0 12] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 4  0  2  4  9  7  6  1  0  2  0  1  0 12] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 4  0  2  4  9  7  6  1  0  2  0  1  0 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 133\n",
      "count: 31\n",
      "start state: [ 4  0  2  4  9  7  6  1  0  2  0  1  0 12] (sum = 48 )\n",
      "learner action: 0\n",
      "intermediate state: [ 0  1  3  5 10  7  6  1  0  2  0  1  0 12] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  1  3  5 10  7  6  1  0  0  1  2  0 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 134\n",
      "count: 32\n",
      "start state: [ 0  1  3  5 10  7  6  1  0  0  1  2  0 12] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  4  5 10  7  6  1  0  0  1  2  0 12] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  0  4  5 10  7  6  1  0  0  1  2  0 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 135\n",
      "count: 33\n",
      "start state: [ 0  0  4  5 10  7  6  1  0  0  1  2  0 12] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  4  5 10  7  6  1  0  0  1  2  0 12] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  0  4  5 10  7  6  1  0  0  0  3  0 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 136\n",
      "count: 34\n",
      "start state: [ 0  0  4  5 10  7  6  1  0  0  0  3  0 12] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  4  5 10  7  6  1  0  0  0  3  0 12] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1  0  4  5 10  7  6  1  0  0  0  0  1 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 137\n",
      "count: 35\n",
      "start state: [ 1  0  4  5 10  7  6  1  0  0  0  0  1 13] (sum = 48 )\n",
      "learner action: 1\n",
      "intermediate state: [ 1  0  4  5 10  7  6  1  0  0  0  0  1 13] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 1  0  4  5 10  7  6  1  0  0  0  0  1 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 138\n",
      "count: 36\n",
      "start state: [ 1  0  4  5 10  7  6  1  0  0  0  0  1 13] (sum = 48 )\n",
      "learner action: 5\n",
      "intermediate state: [ 1  0  4  5 10  0  7  2  1  1  1  1  2 13] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1  0  4  5 10  0  7  2  1  1  1  0  3 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 139\n",
      "count: 37\n",
      "start state: [ 1  0  4  5 10  0  7  2  1  1  1  0  3 13] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 1  0  0  6 11  1  8  2  1  1  1  0  3 13] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  1  0  6 11  1  8  2  1  1  1  0  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 140\n",
      "count: 38\n",
      "start state: [ 2  1  0  6 11  1  8  2  1  1  1  0  0 14] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  1  6 11  1  8  2  1  1  1  0  0 14] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  0  1  6 11  1  8  2  1  1  1  0  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 141\n",
      "count: 39\n",
      "start state: [ 2  0  1  6 11  1  8  2  1  1  1  0  0 14] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  1  6 11  1  8  2  1  1  1  0  0 14] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  0  1  6 11  1  8  2  0  2  1  0  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 142\n",
      "count: 40\n",
      "start state: [ 2  0  1  6 11  1  8  2  0  2  1  0  0 14] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  1  6 11  1  8  2  0  2  1  0  0 14] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 2  0  1  6 11  1  8  2  0  0  2  1  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 143\n",
      "count: 41\n",
      "start state: [ 2  0  1  6 11  1  8  2  0  0  2  1  0 14] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  1  6 11  1  8  2  0  0  2  1  0 14] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  0  1  6 11  1  8  2  0  0  0  2  1 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 144\n",
      "count: 42\n",
      "start state: [ 2  0  1  6 11  1  8  2  0  0  0  2  1 14] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  1  6 11  1  8  2  0  0  0  2  1 14] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 2  0  1  6 11  1  8  2  0  0  0  2  1 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 145\n",
      "count: 43\n",
      "start state: [ 2  0  1  6 11  1  8  2  0  0  0  2  1 14] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  1  6 11  1  8  2  0  0  0  2  1 14] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  0  1  6 11  1  8  2  0  0  0  2  1 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 146\n",
      "count: 44\n",
      "start state: [ 2  0  1  6 11  1  8  2  0  0  0  2  1 14] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  1  6 11  1  8  2  0  0  0  2  1 14] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  0  1  6 11  1  8  2  0  0  0  2  1 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 147\n",
      "count: 45\n",
      "start state: [ 2  0  1  6 11  1  8  2  0  0  0  2  1 14] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  1  6 11  1  8  2  0  0  0  2  1 14] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  0  1  6 11  1  8  0  1  1  0  2  1 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 148\n",
      "count: 46\n",
      "start state: [ 2  0  1  6 11  1  8  0  1  1  0  2  1 14] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  0  0  7 11  1  8  0  1  1  0  2  1 14] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  0  0  7 11  1  8  0  1  1  0  2  1 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 149\n",
      "count: 47\n",
      "start state: [ 2  0  0  7 11  1  8  0  1  1  0  2  1 14] (sum = 48 )\n",
      "learner action: 1\n",
      "intermediate state: [ 2  0  0  7 11  1  8  0  1  1  0  2  1 14] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  0  0  7 11  1  8  0  1  1  0  0  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 150\n",
      "count: 48\n",
      "start state: [ 2  0  0  7 11  1  8  0  1  1  0  0  2 15] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  0  7 11  1  8  0  1  1  0  0  2 15] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  0  0  7 11  1  8  0  1  1  0  0  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 151\n",
      "count: 49\n",
      "start state: [ 2  0  0  7 11  1  8  0  1  1  0  0  2 15] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  0  7 11  1  8  0  1  1  0  0  2 15] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 152\n",
      "count: 50\n",
      "start state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 153\n",
      "count: 51\n",
      "start state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 154\n",
      "count: 52\n",
      "start state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 155\n",
      "count: 53\n",
      "start state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 156\n",
      "count: 54\n",
      "start state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 157\n",
      "count: 55\n",
      "start state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 158\n",
      "count: 56\n",
      "start state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 159\n",
      "count: 57\n",
      "start state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 160\n",
      "count: 58\n",
      "start state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  0  7 11  1  8  0  0  2  0  0  2 15] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 2  0  0  7 11  1  8  0  0  0  1  1  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 161\n",
      "count: 59\n",
      "start state: [ 2  0  0  7 11  1  8  0  0  0  1  1  2 15] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  0  7 11  1  8  0  0  0  1  1  2 15] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 3  0  0  7 11  1  8  0  0  0  1  1  0 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 162\n",
      "count: 60\n",
      "start state: [ 3  0  0  7 11  1  8  0  0  0  1  1  0 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 3  0  0  7 11  1  8  0  0  0  1  1  0 16] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 3  0  0  7 11  1  8  0  0  0  1  1  0 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 163\n",
      "count: 61\n",
      "start state: [ 3  0  0  7 11  1  8  0  0  0  1  1  0 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 3  0  0  7 11  1  8  0  0  0  1  1  0 16] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 3  0  0  7 11  1  8  0  0  0  1  0  1 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 164\n",
      "count: 62\n",
      "start state: [ 3  0  0  7 11  1  8  0  0  0  1  0  1 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 3  0  0  7 11  1  8  0  0  0  1  0  1 16] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 3  0  0  7 11  1  8  0  0  0  0  1  1 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 165\n",
      "count: 63\n",
      "start state: [ 3  0  0  7 11  1  8  0  0  0  0  1  1 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 3  0  0  7 11  1  8  0  0  0  0  1  1 16] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 3  0  0  7 11  1  8  0  0  0  0  1  1 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 166\n",
      "count: 64\n",
      "start state: [ 3  0  0  7 11  1  8  0  0  0  0  1  1 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 3  0  0  7 11  1  8  0  0  0  0  1  1 16] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 3  0  0  7 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 167\n",
      "count: 65\n",
      "start state: [ 3  0  0  7 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: 0\n",
      "intermediate state: [ 0  1  1  8 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  1  1  8 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 168\n",
      "count: 66\n",
      "start state: [ 0  1  1  8 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 0  1  0  9 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  1  0  9 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 169\n",
      "count: 67\n",
      "start state: [ 0  1  0  9 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  1  9 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  0  1  9 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 170\n",
      "count: 68\n",
      "start state: [ 0  0  1  9 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  1  9 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  0  1  9 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 171\n",
      "count: 69\n",
      "start state: [ 0  0  1  9 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: 2\n",
      "intermediate state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 172\n",
      "count: 70\n",
      "start state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 173\n",
      "count: 71\n",
      "start state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 174\n",
      "count: 72\n",
      "start state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 175\n",
      "count: 73\n",
      "start state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 176\n",
      "count: 74\n",
      "start state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: 0\n",
      "intermediate state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 177\n",
      "count: 75\n",
      "start state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 178\n",
      "count: 76\n",
      "start state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 179\n",
      "count: 77\n",
      "start state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 180\n",
      "count: 78\n",
      "start state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 181\n",
      "count: 79\n",
      "start state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 182\n",
      "count: 80\n",
      "start state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 183\n",
      "count: 81\n",
      "start state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 184\n",
      "count: 82\n",
      "start state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 185\n",
      "count: 83\n",
      "start state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 186\n",
      "count: 84\n",
      "start state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 187\n",
      "count: 85\n",
      "start state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 188\n",
      "count: 86\n",
      "start state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  0 10 11  1  8  0  0  0  0  0  2 16] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 189\n",
      "count: 87\n",
      "start state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 190\n",
      "count: 88\n",
      "start state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 191\n",
      "count: 89\n",
      "start state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 192\n",
      "count: 90\n",
      "start state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 193\n",
      "count: 91\n",
      "start state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 194\n",
      "count: 92\n",
      "start state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 195\n",
      "count: 93\n",
      "start state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 196\n",
      "count: 94\n",
      "start state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 197\n",
      "count: 95\n",
      "start state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 198\n",
      "count: 96\n",
      "start state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 199\n",
      "count: 97\n",
      "start state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 200\n",
      "count: 98\n",
      "start state: [ 1  0  0 10 11  1  8  0  0  0  0  0  0 17] (sum = 48 )\n",
      "learner action: 3\n",
      "intermediate state: [ 2  0  0  0 12  2  9  1  1  1  1  1  1 17] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  0  0  0 12  2  9  1  1  1  1  0  2 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 201\n",
      "count: 99\n",
      "start state: [ 2  0  0  0 12  2  9  1  1  1  1  0  2 17] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  0  0 12  2  9  1  1  1  1  0  2 17] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  0  0  0 12  2  9  1  1  1  0  1  2 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "global step: 202\n",
      "count: 100\n",
      "start state: [ 2  0  0  0 12  2  9  1  1  1  0  1  2 17] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0  0  0 12  2  9  1  1  1  0  1  2 17] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  0  0  0 12  2  9  1  0  2  0  1  2 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 1\n",
      "\n",
      "\n",
      "EPISODE 2\n",
      "[4 4 4 4 4 4 0 4 4 4 4 4 4 0]\n",
      "global step: 203\n",
      "count: 0\n",
      "start state: [4 4 4 4 4 4 0 4 4 4 4 4 4 0] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [4 4 4 4 0 5 1 5 5 4 4 4 4 0] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [5 5 5 4 0 5 1 5 5 4 4 4 0 1] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 204\n",
      "count: 1\n",
      "start state: [5 5 5 4 0 5 1 5 5 4 4 4 0 1] (sum = 48 )\n",
      "learner action: 0\n",
      "intermediate state: [0 6 6 5 1 6 1 5 5 4 4 4 0 1] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [1 6 6 5 1 6 1 5 5 4 0 5 1 2] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 205\n",
      "count: 2\n",
      "start state: [1 6 6 5 1 6 1 5 5 4 0 5 1 2] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [1 6 6 5 1 0 2 6 6 5 1 6 1 2] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [2 7 7 6 1 0 2 6 6 5 1 0 2 3] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 206\n",
      "count: 3\n",
      "start state: [2 7 7 6 1 0 2 6 6 5 1 0 2 3] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [2 7 7 6 1 0 2 6 6 5 1 0 2 3] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [2 7 7 6 1 0 2 6 6 5 1 0 2 3] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 207\n",
      "count: 4\n",
      "start state: [2 7 7 6 1 0 2 6 6 5 1 0 2 3] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [2 7 7 6 1 0 2 6 6 5 1 0 2 3] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [2 7 7 6 1 0 2 6 6 5 1 0 2 3] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 208\n",
      "count: 5\n",
      "start state: [2 7 7 6 1 0 2 6 6 5 1 0 2 3] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [2 7 7 6 1 0 2 6 6 5 1 0 2 3] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [2 7 7 6 1 0 2 6 6 5 0 1 2 3] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 209\n",
      "count: 6\n",
      "start state: [2 7 7 6 1 0 2 6 6 5 0 1 2 3] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [2 7 7 6 1 0 2 6 6 5 0 1 2 3] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [3 7 7 6 1 0 2 6 6 0 1 2 3 4] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 210\n",
      "count: 7\n",
      "start state: [3 7 7 6 1 0 2 6 6 0 1 2 3 4] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [3 7 7 6 1 0 2 6 6 0 1 2 3 4] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [3 7 7 6 1 0 2 0 7 1 2 3 4 5] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 211\n",
      "count: 8\n",
      "start state: [3 7 7 6 1 0 2 0 7 1 2 3 4 5] (sum = 48 )\n",
      "learner action: 0\n",
      "intermediate state: [0 8 8 7 1 0 2 0 7 1 2 3 4 5] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [0 8 8 7 1 0 2 0 7 0 3 3 4 5] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 212\n",
      "count: 9\n",
      "start state: [0 8 8 7 1 0 2 0 7 0 3 3 4 5] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [0 8 8 7 0 1 2 0 7 0 3 3 4 5] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [0 8 8 7 0 1 2 0 7 0 0 4 5 6] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 213\n",
      "count: 10\n",
      "start state: [0 8 8 7 0 1 2 0 7 0 0 4 5 6] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [0 8 8 7 0 1 2 0 7 0 0 4 5 6] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [0 8 8 7 0 1 2 0 7 0 0 4 5 6] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 214\n",
      "count: 11\n",
      "start state: [0 8 8 7 0 1 2 0 7 0 0 4 5 6] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [0 8 8 7 0 1 2 0 7 0 0 4 5 6] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [1 9 9 8 0 1 2 0 7 0 0 4 0 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 215\n",
      "count: 12\n",
      "start state: [1 9 9 8 0 1 2 0 7 0 0 4 0 7] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [1 9 9 0 1 2 3 1 8 1 1 5 0 7] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2 10 10  0  1  2  3  1  8  1  1  0  1  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 216\n",
      "count: 13\n",
      "start state: [ 2 10 10  0  1  2  3  1  8  1  1  0  1  8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2 10 10  0  1  0  4  2  8  1  1  0  1  8] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2 10 10  0  1  0  4  2  8  1  0  1  1  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 217\n",
      "count: 14\n",
      "start state: [ 2 10 10  0  1  0  4  2  8  1  0  1  1  8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2 10 10  0  1  0  4  2  8  1  0  1  1  8] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2 10 10  0  1  0  4  2  8  1  0  1  1  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 218\n",
      "count: 15\n",
      "start state: [ 2 10 10  0  1  0  4  2  8  1  0  1  1  8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2 10 10  0  1  0  4  2  8  1  0  1  1  8] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2 10 10  0  1  0  4  2  8  1  0  0  2  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 219\n",
      "count: 16\n",
      "start state: [ 2 10 10  0  1  0  4  2  8  1  0  0  2  8] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2 10 10  0  1  0  4  2  8  1  0  0  2  8] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2 10 10  0  1  0  4  0  9  2  0  0  2  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 220\n",
      "count: 17\n",
      "start state: [ 2 10 10  0  1  0  4  0  9  2  0  0  2  8] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2 10 10  0  1  0  4  0  9  2  0  0  2  8] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 3 10 10  0  1  0  4  0  9  2  0  0  0  9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 221\n",
      "count: 18\n",
      "start state: [ 3 10 10  0  1  0  4  0  9  2  0  0  0  9] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 3 10 10  0  1  0  4  0  9  2  0  0  0  9] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 3 10 10  0  1  0  4  0  9  2  0  0  0  9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 222\n",
      "count: 19\n",
      "start state: [ 3 10 10  0  1  0  4  0  9  2  0  0  0  9] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 3 10 10  0  1  0  4  0  9  2  0  0  0  9] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 3 10 10  0  1  0  4  0  9  2  0  0  0  9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 223\n",
      "count: 20\n",
      "start state: [ 3 10 10  0  1  0  4  0  9  2  0  0  0  9] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 3 10 10  0  1  0  4  0  9  2  0  0  0  9] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 3 10 10  0  1  0  4  0  9  2  0  0  0  9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 224\n",
      "count: 21\n",
      "start state: [ 3 10 10  0  1  0  4  0  9  2  0  0  0  9] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 3 10 10  0  1  0  4  0  9  2  0  0  0  9] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 3 10 10  0  1  0  4  0  9  2  0  0  0  9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 225\n",
      "count: 22\n",
      "start state: [ 3 10 10  0  1  0  4  0  9  2  0  0  0  9] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 3 10 10  0  1  0  4  0  9  2  0  0  0  9] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 3 10 10  0  1  0  4  0  9  0  1  1  0  9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 226\n",
      "count: 23\n",
      "start state: [ 3 10 10  0  1  0  4  0  9  0  1  1  0  9] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 3 10 10  0  1  0  4  0  9  0  1  1  0  9] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 4 11 11  1  1  0  4  0  0  1  2  2  1 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 227\n",
      "count: 24\n",
      "start state: [ 4 11 11  1  1  0  4  0  0  1  2  2  1 10] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 4 11 11  1  1  0  4  0  0  1  2  2  1 10] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 4 11 11  1  1  0  4  0  0  1  2  2  0 11] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 228\n",
      "count: 25\n",
      "start state: [ 4 11 11  1  1  0  4  0  0  1  2  2  0 11] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 4 11 11  1  1  0  4  0  0  1  2  2  0 11] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 4 11 11  1  1  0  4  0  0  1  2  0  1 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 229\n",
      "count: 26\n",
      "start state: [ 4 11 11  1  1  0  4  0  0  1  2  0  1 12] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 4 11 11  0  2  0  4  0  0  1  2  0  1 12] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 230\n",
      "count: 27\n",
      "start state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 231\n",
      "count: 28\n",
      "start state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "learner action: 3\n",
      "intermediate state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 232\n",
      "count: 29\n",
      "start state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 233\n",
      "count: 30\n",
      "start state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 234\n",
      "count: 31\n",
      "start state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 235\n",
      "count: 32\n",
      "start state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 236\n",
      "count: 33\n",
      "start state: [ 4 11 11  0  2  0  4  0  0  0  3  0  1 12] (sum = 48 )\n",
      "learner action: 1\n",
      "intermediate state: [ 4  0 12  1  3  1  5  1  1  1  4  1  2 12] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 4  0 12  1  3  1  5  0  2  1  4  1  2 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 237\n",
      "count: 34\n",
      "start state: [ 4  0 12  1  3  1  5  0  2  1  4  1  2 12] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 4  0 12  0  4  1  5  0  2  1  4  1  2 12] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 4  0 12  0  4  1  5  0  2  1  4  1  2 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 238\n",
      "count: 35\n",
      "start state: [ 4  0 12  0  4  1  5  0  2  1  4  1  2 12] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 4  0 12  0  4  1  5  0  2  1  4  1  2 12] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 5  0 12  0  4  1  5  0  2  1  0  2  3 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 239\n",
      "count: 36\n",
      "start state: [ 5  0 12  0  4  1  5  0  2  1  0  2  3 13] (sum = 48 )\n",
      "learner action: 0\n",
      "intermediate state: [ 0  1 13  1  5  2  5  0  2  1  0  2  3 13] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  1 13  1  5  2  5  0  2  1  0  0  4 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 240\n",
      "count: 37\n",
      "start state: [ 0  1 13  1  5  2  5  0  2  1  0  0  4 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  1 13  0  6  2  5  0  2  1  0  0  4 14] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  1 13  0  6  2  5  0  2  1  0  0  4 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 241\n",
      "count: 38\n",
      "start state: [ 0  1 13  0  6  2  5  0  2  1  0  0  4 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  1 13  0  6  2  5  0  2  1  0  0  4 14] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 242\n",
      "count: 39\n",
      "start state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 243\n",
      "count: 40\n",
      "start state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 244\n",
      "count: 41\n",
      "start state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 245\n",
      "count: 42\n",
      "start state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 246\n",
      "count: 43\n",
      "start state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 247\n",
      "count: 44\n",
      "start state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 248\n",
      "count: 45\n",
      "start state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 14  0  6  2  5  0  2  1  0  0  0 15] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  2 14  0  6  2  5  0  0  2  1  0  0 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 249\n",
      "count: 46\n",
      "start state: [ 1  2 14  0  6  2  5  0  0  2  1  0  0 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 14  0  6  2  5  0  0  2  1  0  0 15] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 1  2 14  0  6  2  5  0  0  0  2  1  0 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 250\n",
      "count: 47\n",
      "start state: [ 1  2 14  0  6  2  5  0  0  0  2  1  0 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 14  0  6  2  5  0  0  0  2  1  0 15] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  2 14  0  6  2  5  0  0  0  2  1  0 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 251\n",
      "count: 48\n",
      "start state: [ 1  2 14  0  6  2  5  0  0  0  2  1  0 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 14  0  6  2  5  0  0  0  2  1  0 15] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 252\n",
      "count: 49\n",
      "start state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 253\n",
      "count: 50\n",
      "start state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 254\n",
      "count: 51\n",
      "start state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 255\n",
      "count: 52\n",
      "start state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 256\n",
      "count: 53\n",
      "start state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 257\n",
      "count: 54\n",
      "start state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 258\n",
      "count: 55\n",
      "start state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 14  0  6  2  5  0  0  0  2  0  1 15] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 1  2 14  0  6  2  5  0  0  0  0  1  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 259\n",
      "count: 56\n",
      "start state: [ 1  2 14  0  6  2  5  0  0  0  0  1  2 15] (sum = 48 )\n",
      "learner action: 1\n",
      "intermediate state: [ 1  0 15  1  6  2  5  0  0  0  0  1  2 15] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 1  0 15  1  6  2  5  0  0  0  0  1  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 260\n",
      "count: 57\n",
      "start state: [ 1  0 15  1  6  2  5  0  0  0  0  1  2 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  0 15  0  7  2  5  0  0  0  0  1  2 15] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 1  0 15  0  7  2  5  0  0  0  0  1  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 261\n",
      "count: 58\n",
      "start state: [ 1  0 15  0  7  2  5  0  0  0  0  1  2 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  0 15  0  7  2  5  0  0  0  0  1  2 15] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 1  0 15  0  7  2  5  0  0  0  0  1  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 262\n",
      "count: 59\n",
      "start state: [ 1  0 15  0  7  2  5  0  0  0  0  1  2 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  0 15  0  7  2  5  0  0  0  0  1  2 15] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  0 15  0  7  2  5  0  0  0  0  1  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 263\n",
      "count: 60\n",
      "start state: [ 1  0 15  0  7  2  5  0  0  0  0  1  2 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  0 15  0  7  2  5  0  0  0  0  1  2 15] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 1  0 15  0  7  2  5  0  0  0  0  1  2 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 264\n",
      "count: 61\n",
      "start state: [ 1  0 15  0  7  2  5  0  0  0  0  1  2 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  0 15  0  7  2  5  0  0  0  0  1  2 15] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  0 15  0  7  2  5  0  0  0  0  1  0 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 265\n",
      "count: 62\n",
      "start state: [ 2  0 15  0  7  2  5  0  0  0  0  1  0 16] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  0 15  0  7  2  5  0  0  0  0  1  0 16] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  0 15  0  7  2  5  0  0  0  0  0  1 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 266\n",
      "count: 63\n",
      "start state: [ 2  0 15  0  7  2  5  0  0  0  0  0  1 16] (sum = 48 )\n",
      "learner action: 4\n",
      "intermediate state: [ 2  0 15  0  0  3  6  1  1  1  1  1  1 16] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  0 15  0  0  3  6  1  1  1  1  0  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 267\n",
      "count: 64\n",
      "start state: [ 2  0 15  0  0  3  6  1  1  1  1  0  2 16] (sum = 48 )\n",
      "learner action: 3\n",
      "intermediate state: [ 2  0 15  0  0  3  6  1  1  1  1  0  2 16] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  0 15  0  0  3  6  1  1  1  0  1  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 268\n",
      "count: 65\n",
      "start state: [ 2  0 15  0  0  3  6  1  1  1  0  1  2 16] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  0 15  0  0  3  6  1  1  1  0  1  2 16] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  0 15  0  0  3  6  0  2  1  0  1  2 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 269\n",
      "count: 66\n",
      "start state: [ 2  0 15  0  0  3  6  0  2  1  0  1  2 16] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  0 15  0  0  3  6  0  2  1  0  1  2 16] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  0 15  0  0  3  6  0  2  1  0  0  3 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 270\n",
      "count: 67\n",
      "start state: [ 2  0 15  0  0  3  6  0  2  1  0  0  3 16] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  0 15  0  0  3  6  0  2  1  0  0  3 16] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 2  0 15  0  0  3  6  0  2  0  1  0  3 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 271\n",
      "count: 68\n",
      "start state: [ 2  0 15  0  0  3  6  0  2  0  1  0  3 16] (sum = 48 )\n",
      "learner action: 4\n",
      "intermediate state: [ 2  0 15  0  0  3  6  0  2  0  1  0  3 16] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  0 15  0  0  3  6  0  2  0  1  0  3 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 272\n",
      "count: 69\n",
      "start state: [ 2  0 15  0  0  3  6  0  2  0  1  0  3 16] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  0 15  0  0  3  6  0  2  0  1  0  3 16] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  0 15  0  0  3  6  0  2  0  1  0  3 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 273\n",
      "count: 70\n",
      "start state: [ 2  0 15  0  0  3  6  0  2  0  1  0  3 16] (sum = 48 )\n",
      "learner action: 4\n",
      "intermediate state: [ 2  0 15  0  0  3  6  0  2  0  1  0  3 16] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  0 15  0  0  3  6  0  2  0  1  0  3 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 274\n",
      "count: 71\n",
      "start state: [ 2  0 15  0  0  3  6  0  2  0  1  0  3 16] (sum = 48 )\n",
      "learner action: 0\n",
      "intermediate state: [ 0  1 16  0  0  3  6  0  2  0  1  0  3 16] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  1 16  0  0  3  6  0  2  0  0  1  3 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 275\n",
      "count: 72\n",
      "start state: [ 0  1 16  0  0  3  6  0  2  0  0  1  3 16] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  1 16  0  0  3  6  0  2  0  0  1  3 16] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  1 16  0  0  3  6  0  2  0  0  0  4 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 276\n",
      "count: 73\n",
      "start state: [ 0  1 16  0  0  3  6  0  2  0  0  0  4 16] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  1 16  0  0  3  6  0  2  0  0  0  4 16] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  1 16  0  0  3  6  0  2  0  0  0  4 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 277\n",
      "count: 74\n",
      "start state: [ 0  1 16  0  0  3  6  0  2  0  0  0  4 16] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  1 16  0  0  3  6  0  2  0  0  0  4 16] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  1 16  0  0  3  6  0  2  0  0  0  4 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 278\n",
      "count: 75\n",
      "start state: [ 0  1 16  0  0  3  6  0  2  0  0  0  4 16] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  1 16  0  0  3  6  0  2  0  0  0  4 16] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 1  2 17  0  0  3  6  0  2  0  0  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 279\n",
      "count: 76\n",
      "start state: [ 1  2 17  0  0  3  6  0  2  0  0  0  0 17] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 17  0  0  3  6  0  2  0  0  0  0 17] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  2 17  0  0  3  6  0  2  0  0  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 280\n",
      "count: 77\n",
      "start state: [ 1  2 17  0  0  3  6  0  2  0  0  0  0 17] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 17  0  0  3  6  0  2  0  0  0  0 17] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  2 17  0  0  3  6  0  2  0  0  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 281\n",
      "count: 78\n",
      "start state: [ 1  2 17  0  0  3  6  0  2  0  0  0  0 17] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 17  0  0  3  6  0  2  0  0  0  0 17] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  2 17  0  0  3  6  0  0  1  1  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 282\n",
      "count: 79\n",
      "start state: [ 1  2 17  0  0  3  6  0  0  1  1  0  0 17] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 17  0  0  3  6  0  0  1  1  0  0 17] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  2 17  0  0  3  6  0  0  1  1  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 283\n",
      "count: 80\n",
      "start state: [ 1  2 17  0  0  3  6  0  0  1  1  0  0 17] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 17  0  0  3  6  0  0  1  1  0  0 17] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  2 17  0  0  3  6  0  0  1  1  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 284\n",
      "count: 81\n",
      "start state: [ 1  2 17  0  0  3  6  0  0  1  1  0  0 17] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 17  0  0  3  6  0  0  1  1  0  0 17] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  2 17  0  0  3  6  0  0  1  1  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 285\n",
      "count: 82\n",
      "start state: [ 1  2 17  0  0  3  6  0  0  1  1  0  0 17] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 17  0  0  3  6  0  0  1  1  0  0 17] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  2 17  0  0  3  6  0  0  1  1  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 286\n",
      "count: 83\n",
      "start state: [ 1  2 17  0  0  3  6  0  0  1  1  0  0 17] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 17  0  0  3  6  0  0  1  1  0  0 17] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1  2 17  0  0  3  6  0  0  1  1  0  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 287\n",
      "count: 84\n",
      "start state: [ 1  2 17  0  0  3  6  0  0  1  1  0  0 17] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 17  0  0  3  6  0  0  1  1  0  0 17] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 1  2 17  0  0  3  6  0  0  1  0  1  0 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 288\n",
      "count: 85\n",
      "start state: [ 1  2 17  0  0  3  6  0  0  1  0  1  0 17] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 17  0  0  3  6  0  0  1  0  1  0 17] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1  2 17  0  0  3  6  0  0  1  0  0  1 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 289\n",
      "count: 86\n",
      "start state: [ 1  2 17  0  0  3  6  0  0  1  0  0  1 17] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 17  0  0  3  6  0  0  1  0  0  1 17] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  2 17  0  0  3  6  0  0  1  0  0  1 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 290\n",
      "count: 87\n",
      "start state: [ 1  2 17  0  0  3  6  0  0  1  0  0  1 17] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 17  0  0  3  6  0  0  1  0  0  1 17] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 1  2 17  0  0  3  6  0  0  0  1  0  1 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 291\n",
      "count: 88\n",
      "start state: [ 1  2 17  0  0  3  6  0  0  0  1  0  1 17] (sum = 48 )\n",
      "learner action: 3\n",
      "intermediate state: [ 1  2 17  0  0  3  6  0  0  0  1  0  1 17] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 1  2 17  0  0  3  6  0  0  0  1  0  1 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 292\n",
      "count: 89\n",
      "start state: [ 1  2 17  0  0  3  6  0  0  0  1  0  1 17] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 17  0  0  3  6  0  0  0  1  0  1 17] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 1  2 17  0  0  3  6  0  0  0  1  0  1 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 293\n",
      "count: 90\n",
      "start state: [ 1  2 17  0  0  3  6  0  0  0  1  0  1 17] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 17  0  0  3  6  0  0  0  1  0  1 17] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 1  2 17  0  0  3  6  0  0  0  0  1  1 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 294\n",
      "count: 91\n",
      "start state: [ 1  2 17  0  0  3  6  0  0  0  0  1  1 17] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 17  0  0  3  6  0  0  0  0  1  1 17] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  2 17  0  0  3  6  0  0  0  0  1  1 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 295\n",
      "count: 92\n",
      "start state: [ 1  2 17  0  0  3  6  0  0  0  0  1  1 17] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 17  0  0  3  6  0  0  0  0  1  1 17] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1  2 17  0  0  3  6  0  0  0  0  0  2 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 296\n",
      "count: 93\n",
      "start state: [ 1  2 17  0  0  3  6  0  0  0  0  0  2 17] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  2 17  0  0  3  6  0  0  0  0  0  2 17] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  2 17  0  0  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 297\n",
      "count: 94\n",
      "start state: [ 2  2 17  0  0  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  2 17  0  0  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  2 17  0  0  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 298\n",
      "count: 95\n",
      "start state: [ 2  2 17  0  0  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  2 17  0  0  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  2 17  0  0  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 299\n",
      "count: 96\n",
      "start state: [ 2  2 17  0  0  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  2 17  0  0  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  2 17  0  0  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 300\n",
      "count: 97\n",
      "start state: [ 2  2 17  0  0  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  2 17  0  0  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  2 17  0  0  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 301\n",
      "count: 98\n",
      "start state: [ 2  2 17  0  0  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "learner action: 1\n",
      "intermediate state: [ 2  0 18  1  0  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  0 18  1  0  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 302\n",
      "count: 99\n",
      "start state: [ 2  0 18  1  0  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  0 18  0  1  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  0 18  0  1  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 303\n",
      "count: 100\n",
      "start state: [ 2  0 18  0  1  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  0 18  0  1  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  0 18  0  1  3  6  0  0  0  0  0  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "EPISODE 3\n",
      "[4 4 4 4 4 4 0 4 4 4 4 4 4 0]\n",
      "global step: 304\n",
      "count: 0\n",
      "start state: [4 4 4 4 4 4 0 4 4 4 4 4 4 0] (sum = 48 )\n",
      "learner action: 4\n",
      "intermediate state: [4 4 4 4 0 5 1 5 5 4 4 4 4 0] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [4 4 4 4 0 5 1 5 0 5 5 5 5 1] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 305\n",
      "count: 1\n",
      "start state: [4 4 4 4 0 5 1 5 0 5 5 5 5 1] (sum = 48 )\n",
      "learner action: 1\n",
      "intermediate state: [4 0 5 5 1 6 1 5 0 5 5 5 5 1] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [4 0 5 5 1 6 1 0 1 6 6 6 6 1] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 306\n",
      "count: 2\n",
      "start state: [4 0 5 5 1 6 1 0 1 6 6 6 6 1] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [4 0 5 5 0 7 1 0 1 6 6 6 6 1] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [5 1 6 6 1 7 1 0 1 6 6 6 0 2] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 307\n",
      "count: 3\n",
      "start state: [5 1 6 6 1 7 1 0 1 6 6 6 0 2] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [0 2 7 7 2 8 1 0 1 6 6 6 0 2] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [0 2 7 7 2 8 1 0 1 6 6 6 0 2] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 308\n",
      "count: 4\n",
      "start state: [0 2 7 7 2 8 1 0 1 6 6 6 0 2] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [0 0 8 8 2 8 1 0 1 6 6 6 0 2] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [0 0 8 8 2 8 1 0 1 6 6 6 0 2] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 309\n",
      "count: 5\n",
      "start state: [0 0 8 8 2 8 1 0 1 6 6 6 0 2] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [0 0 8 8 2 8 1 0 1 6 6 6 0 2] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [1 1 9 9 2 8 1 0 1 6 6 0 1 3] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 310\n",
      "count: 6\n",
      "start state: [1 1 9 9 2 8 1 0 1 6 6 0 1 3] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0 10  9  2  8  1  0  1  6  6  0  1  3] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 2  1 10  9  2  8  1  0  1  0  7  1  2  4] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 311\n",
      "count: 7\n",
      "start state: [ 2  1 10  9  2  8  1  0  1  0  7  1  2  4] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0 11  9  2  8  1  0  1  0  7  1  2  4] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 3  0 11  9  2  8  1  0  1  0  7  1  0  5] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 312\n",
      "count: 8\n",
      "start state: [ 3  0 11  9  2  8  1  0  1  0  7  1  0  5] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 3  0 11  9  2  8  1  0  1  0  7  1  0  5] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 4  1 12 10  2  8  1  0  1  0  0  2  1  6] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 313\n",
      "count: 9\n",
      "start state: [ 4  1 12 10  2  8  1  0  1  0  0  2  1  6] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 4  0 13 10  2  8  1  0  1  0  0  2  1  6] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 4  0 13 10  2  8  1  0  1  0  0  2  1  6] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 314\n",
      "count: 10\n",
      "start state: [ 4  0 13 10  2  8  1  0  1  0  0  2  1  6] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 4  0 13 10  2  8  1  0  1  0  0  2  1  6] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 4  0 13 10  2  8  1  0  1  0  0  2  1  6] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 315\n",
      "count: 11\n",
      "start state: [ 4  0 13 10  2  8  1  0  1  0  0  2  1  6] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 4  0 13 10  2  8  1  0  1  0  0  2  1  6] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 4  0 13 10  2  8  1  0  0  1  0  2  1  6] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 316\n",
      "count: 12\n",
      "start state: [ 4  0 13 10  2  8  1  0  0  1  0  2  1  6] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 4  0 13 10  2  8  1  0  0  1  0  2  1  6] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 4  0 13 10  2  8  1  0  0  1  0  2  1  6] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 317\n",
      "count: 13\n",
      "start state: [ 4  0 13 10  2  8  1  0  0  1  0  2  1  6] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 4  0 13 10  2  8  1  0  0  1  0  2  1  6] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 4  0 13 10  2  8  1  0  0  1  0  2  1  6] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 318\n",
      "count: 14\n",
      "start state: [ 4  0 13 10  2  8  1  0  0  1  0  2  1  6] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 4  0 13 10  2  8  1  0  0  1  0  2  1  6] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 4  0 13 10  2  8  1  0  0  1  0  2  1  6] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 319\n",
      "count: 15\n",
      "start state: [ 4  0 13 10  2  8  1  0  0  1  0  2  1  6] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 4  0 13 10  2  8  1  0  0  1  0  2  1  6] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 4  0 13 10  2  8  1  0  0  1  0  2  1  6] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 320\n",
      "count: 16\n",
      "start state: [ 4  0 13 10  2  8  1  0  0  1  0  2  1  6] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 4  0 13 10  2  8  1  0  0  1  0  2  1  6] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 4  0 13 10  2  8  1  0  0  1  0  2  0  7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 321\n",
      "count: 17\n",
      "start state: [ 4  0 13 10  2  8  1  0  0  1  0  2  0  7] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 4  0 13 10  2  8  1  0  0  1  0  2  0  7] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 4  0 13 10  2  8  1  0  0  1  0  0  1  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 322\n",
      "count: 18\n",
      "start state: [ 4  0 13 10  2  8  1  0  0  1  0  0  1  8] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  1 14 11  3  8  1  0  0  1  0  0  1  8] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  1 14 11  3  8  1  0  0  1  0  0  1  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 323\n",
      "count: 19\n",
      "start state: [ 0  1 14 11  3  8  1  0  0  1  0  0  1  8] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0 15 11  3  8  1  0  0  1  0  0  1  8] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  0 15 11  3  8  1  0  0  1  0  0  1  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 324\n",
      "count: 20\n",
      "start state: [ 0  0 15 11  3  8  1  0  0  1  0  0  1  8] (sum = 48 )\n",
      "learner action: 5\n",
      "intermediate state: [ 1  0 15 11  3  0  2  1  1  2  1  1  2  8] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  0 15 11  3  0  2  0  2  2  1  1  2  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 325\n",
      "count: 21\n",
      "start state: [ 1  0 15 11  3  0  2  0  2  2  1  1  2  8] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  1 15  0  4  1  3  1  3  3  2  2  3  8] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  1 15  0  4  1  3  1  3  3  0  3  4  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 326\n",
      "count: 22\n",
      "start state: [ 2  1 15  0  4  1  3  1  3  3  0  3  4  8] (sum = 48 )\n",
      "learner action: 5\n",
      "intermediate state: [ 2  1 15  0  4  0  4  1  3  3  0  3  4  8] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 3  1 15  0  4  0  4  1  3  3  0  0  5  9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 327\n",
      "count: 23\n",
      "start state: [ 3  1 15  0  4  0  4  1  3  3  0  0  5  9] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 3  1 15  0  4  0  4  1  3  3  0  0  5  9] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 3  1 15  0  4  0  4  1  0  4  1  1  5  9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 328\n",
      "count: 24\n",
      "start state: [ 3  1 15  0  4  0  4  1  0  4  1  1  5  9] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 3  1 15  0  4  0  4  1  0  4  1  1  5  9] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 3  1 15  0  4  0  4  1  0  4  1  1  5  9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 329\n",
      "count: 25\n",
      "start state: [ 3  1 15  0  4  0  4  1  0  4  1  1  5  9] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 3  1 15  0  4  0  4  1  0  4  1  1  5  9] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 3  1 15  0  4  0  4  1  0  4  1  0  6  9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 330\n",
      "count: 26\n",
      "start state: [ 3  1 15  0  4  0  4  1  0  4  1  0  6  9] (sum = 48 )\n",
      "learner action: 0\n",
      "intermediate state: [ 0  2 16  1  4  0  4  1  0  4  1  0  6  9] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  2 16  1  4  0  4  1  0  4  1  0  6  9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 331\n",
      "count: 27\n",
      "start state: [ 0  2 16  1  4  0  4  1  0  4  1  0  6  9] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  2 16  0  5  0  4  1  0  4  1  0  6  9] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  2 16  0  5  0  4  1  0  4  1  0  6  9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 332\n",
      "count: 28\n",
      "start state: [ 0  2 16  0  5  0  4  1  0  4  1  0  6  9] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  2 16  0  5  0  4  1  0  4  1  0  6  9] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  2 16  0  5  0  4  0  1  4  1  0  6  9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 333\n",
      "count: 29\n",
      "start state: [ 0  2 16  0  5  0  4  0  1  4  1  0  6  9] (sum = 48 )\n",
      "learner action: 3\n",
      "intermediate state: [ 0  2 16  0  5  0  4  0  1  4  1  0  6  9] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 1  3 17  1  6  0  4  0  1  4  1  0  0 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 334\n",
      "count: 30\n",
      "start state: [ 1  3 17  1  6  0  4  0  1  4  1  0  0 10] (sum = 48 )\n",
      "learner action: 4\n",
      "intermediate state: [ 1  3 17  1  0  1  5  1  2  5  2  0  0 10] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 1  3 17  1  0  1  5  1  2  5  2  0  0 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 335\n",
      "count: 31\n",
      "start state: [ 1  3 17  1  0  1  5  1  2  5  2  0  0 10] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  3 17  0  1  1  5  1  2  5  2  0  0 10] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 1  3 17  0  1  1  5  1  2  5  2  0  0 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 336\n",
      "count: 32\n",
      "start state: [ 1  3 17  0  1  1  5  1  2  5  2  0  0 10] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  3 17  0  1  1  5  1  2  5  2  0  0 10] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  3 17  0  1  1  5  1  0  6  3  0  0 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 337\n",
      "count: 33\n",
      "start state: [ 1  3 17  0  1  1  5  1  0  6  3  0  0 10] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  3 17  0  1  1  5  1  0  6  3  0  0 10] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  3 17  0  1  1  5  0  1  6  3  0  0 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 338\n",
      "count: 34\n",
      "start state: [ 1  3 17  0  1  1  5  0  1  6  3  0  0 10] (sum = 48 )\n",
      "learner action: 0\n",
      "intermediate state: [ 0  4 17  0  1  1  5  0  1  6  3  0  0 10] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  4 17  0  1  1  5  0  1  6  3  0  0 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 339\n",
      "count: 35\n",
      "start state: [ 0  4 17  0  1  1  5  0  1  6  3  0  0 10] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  4 17  0  1  1  5  0  1  6  3  0  0 10] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0  4 17  0  1  1  5  0  1  6  3  0  0 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 340\n",
      "count: 36\n",
      "start state: [ 0  4 17  0  1  1  5  0  1  6  3  0  0 10] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  4 17  0  1  1  5  0  1  6  3  0  0 10] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 1  5 17  0  1  1  5  0  1  0  4  1  1 11] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 341\n",
      "count: 37\n",
      "start state: [ 1  5 17  0  1  1  5  0  1  0  4  1  1 11] (sum = 48 )\n",
      "learner action: 3\n",
      "intermediate state: [ 1  5 17  0  1  1  5  0  1  0  4  1  1 11] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  5 17  0  1  1  5  0  1  0  4  1  1 11] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 342\n",
      "count: 38\n",
      "start state: [ 1  5 17  0  1  1  5  0  1  0  4  1  1 11] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  5 17  0  1  1  5  0  1  0  4  1  1 11] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  5 17  0  1  1  5  0  1  0  4  1  1 11] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 343\n",
      "count: 39\n",
      "start state: [ 1  5 17  0  1  1  5  0  1  0  4  1  1 11] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  5 17  0  1  1  5  0  1  0  4  1  1 11] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  5 17  0  1  1  5  0  0  1  4  1  1 11] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 344\n",
      "count: 40\n",
      "start state: [ 1  5 17  0  1  1  5  0  0  1  4  1  1 11] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  5 17  0  1  1  5  0  0  1  4  1  1 11] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 1  5 17  0  1  1  5  0  0  1  4  1  0 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 345\n",
      "count: 41\n",
      "start state: [ 1  5 17  0  1  1  5  0  0  1  4  1  0 12] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  5 17  0  1  1  5  0  0  1  4  1  0 12] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  5 17  0  1  1  5  0  0  1  0  2  1 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 346\n",
      "count: 42\n",
      "start state: [ 2  5 17  0  1  1  5  0  0  1  0  2  1 13] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  1  1  5  0  0  1  0  2  1 13] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 2  5 17  0  1  1  5  0  0  0  1  2  1 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 347\n",
      "count: 43\n",
      "start state: [ 2  5 17  0  1  1  5  0  0  0  1  2  1 13] (sum = 48 )\n",
      "learner action: 4\n",
      "intermediate state: [ 2  5 17  0  0  2  5  0  0  0  1  2  1 13] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  5 17  0  0  2  5  0  0  0  1  2  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 348\n",
      "count: 44\n",
      "start state: [ 2  5 17  0  0  2  5  0  0  0  1  2  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  2  5  0  0  0  1  2  0 14] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 2  5 17  0  0  2  5  0  0  0  1  2  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 349\n",
      "count: 45\n",
      "start state: [ 2  5 17  0  0  2  5  0  0  0  1  2  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  2  5  0  0  0  1  2  0 14] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 350\n",
      "count: 46\n",
      "start state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 351\n",
      "count: 47\n",
      "start state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: 3\n",
      "intermediate state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 352\n",
      "count: 48\n",
      "start state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 353\n",
      "count: 49\n",
      "start state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 354\n",
      "count: 50\n",
      "start state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 355\n",
      "count: 51\n",
      "start state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: 3\n",
      "intermediate state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 356\n",
      "count: 52\n",
      "start state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 357\n",
      "count: 53\n",
      "start state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 358\n",
      "count: 54\n",
      "start state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 359\n",
      "count: 55\n",
      "start state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 360\n",
      "count: 56\n",
      "start state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 361\n",
      "count: 57\n",
      "start state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 362\n",
      "count: 58\n",
      "start state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: 4\n",
      "intermediate state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 363\n",
      "count: 59\n",
      "start state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 364\n",
      "count: 60\n",
      "start state: [ 2  5 17  0  0  2  5  0  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  1  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  5 17  0  0  0  6  1  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 365\n",
      "count: 61\n",
      "start state: [ 2  5 17  0  0  0  6  1  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  1  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  5 17  0  0  0  6  1  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 366\n",
      "count: 62\n",
      "start state: [ 2  5 17  0  0  0  6  1  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  1  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  5 17  0  0  0  6  1  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 367\n",
      "count: 63\n",
      "start state: [ 2  5 17  0  0  0  6  1  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  1  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  5 17  0  0  0  6  1  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 368\n",
      "count: 64\n",
      "start state: [ 2  5 17  0  0  0  6  1  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  1  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  5 17  0  0  0  6  1  0  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 369\n",
      "count: 65\n",
      "start state: [ 2  5 17  0  0  0  6  1  0  0  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  1  0  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  5 17  0  0  0  6  0  1  0  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 370\n",
      "count: 66\n",
      "start state: [ 2  5 17  0  0  0  6  0  1  0  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  0  1  0  0  3  0 14] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  5 17  0  0  0  6  0  0  1  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 371\n",
      "count: 67\n",
      "start state: [ 2  5 17  0  0  0  6  0  0  1  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  0  0  1  0  3  0 14] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  5 17  0  0  0  6  0  0  1  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 372\n",
      "count: 68\n",
      "start state: [ 2  5 17  0  0  0  6  0  0  1  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  0  0  1  0  3  0 14] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  5 17  0  0  0  6  0  0  1  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 373\n",
      "count: 69\n",
      "start state: [ 2  5 17  0  0  0  6  0  0  1  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  0  0  1  0  3  0 14] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  5 17  0  0  0  6  0  0  1  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 374\n",
      "count: 70\n",
      "start state: [ 2  5 17  0  0  0  6  0  0  1  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  0  0  1  0  3  0 14] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  5 17  0  0  0  6  0  0  1  0  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 375\n",
      "count: 71\n",
      "start state: [ 2  5 17  0  0  0  6  0  0  1  0  3  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  0  0  1  0  3  0 14] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 2  5 17  0  0  0  6  0  0  0  1  3  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 376\n",
      "count: 72\n",
      "start state: [ 2  5 17  0  0  0  6  0  0  0  1  3  0 14] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  0  0  0  1  3  0 14] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 377\n",
      "count: 73\n",
      "start state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 378\n",
      "count: 74\n",
      "start state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 379\n",
      "count: 75\n",
      "start state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 380\n",
      "count: 76\n",
      "start state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 381\n",
      "count: 77\n",
      "start state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 382\n",
      "count: 78\n",
      "start state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 383\n",
      "count: 79\n",
      "start state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 384\n",
      "count: 80\n",
      "start state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 385\n",
      "count: 81\n",
      "start state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 386\n",
      "count: 82\n",
      "start state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 387\n",
      "count: 83\n",
      "start state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 388\n",
      "count: 84\n",
      "start state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 2  5 17  0  0  0  6  0  0  0  0  4  0 14] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 3  6 17  0  0  0  6  0  0  0  0  0  1 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 389\n",
      "count: 85\n",
      "start state: [ 3  6 17  0  0  0  6  0  0  0  0  0  1 15] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  7 18  1  0  0  6  0  0  0  0  0  1 15] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  7 18  1  0  0  6  0  0  0  0  0  1 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 390\n",
      "count: 86\n",
      "start state: [ 0  7 18  1  0  0  6  0  0  0  0  0  1 15] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  7 18  0  1  0  6  0  0  0  0  0  1 15] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  7 18  0  1  0  6  0  0  0  0  0  1 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 391\n",
      "count: 87\n",
      "start state: [ 0  7 18  0  1  0  6  0  0  0  0  0  1 15] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 0  7 18  0  0  1  6  0  0  0  0  0  1 15] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0  7 18  0  0  1  6  0  0  0  0  0  0 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 392\n",
      "count: 88\n",
      "start state: [ 0  7 18  0  0  1  6  0  0  0  0  0  0 16] (sum = 48 )\n",
      "learner action: 2\n",
      "intermediate state: [ 1  8  1  2  2  3  8  2  1  1  1  1  1 16] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 1  8  1  2  2  3  8  2  1  1  0  2  1 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 393\n",
      "count: 89\n",
      "start state: [ 1  8  1  2  2  3  8  2  1  1  0  2  1 16] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 1  8  1  2  0  4  9  2  1  1  0  2  1 16] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  8  1  2  0  4  9  0  2  2  0  2  1 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 394\n",
      "count: 90\n",
      "start state: [ 1  8  1  2  0  4  9  0  2  2  0  2  1 16] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 1  8  1  2  0  4  9  0  2  2  0  2  1 16] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1  8  1  2  0  4  9  0  2  2  0  0  2 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 395\n",
      "count: 91\n",
      "start state: [ 1  8  1  2  0  4  9  0  2  2  0  0  2 17] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 1  8  1  2  0  4  9  0  2  2  0  0  2 17] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  8  1  2  0  4  9  0  2  2  0  0  2 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 396\n",
      "count: 92\n",
      "start state: [ 1  8  1  2  0  4  9  0  2  2  0  0  2 17] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 1  8  1  2  0  4  9  0  2  2  0  0  2 17] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  8  1  2  0  4  9  0  0  3  1  0  2 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 397\n",
      "count: 93\n",
      "start state: [ 1  8  1  2  0  4  9  0  0  3  1  0  2 17] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 1  8  1  2  0  4  9  0  0  3  1  0  2 17] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 1  8  1  2  0  4  9  0  0  3  0  1  2 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 398\n",
      "count: 94\n",
      "start state: [ 1  8  1  2  0  4  9  0  0  3  0  1  2 17] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 1  8  1  2  0  4  9  0  0  3  0  1  2 17] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 1  8  1  2  0  4  9  0  0  0  1  2  3 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 399\n",
      "count: 95\n",
      "start state: [ 1  8  1  2  0  4  9  0  0  0  1  2  3 17] (sum = 48 )\n",
      "learner action: 5\n",
      "intermediate state: [ 1  8  1  2  0  0 10  1  1  1  1  2  3 17] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 1  8  1  2  0  0 10  1  1  1  0  3  3 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 400\n",
      "count: 96\n",
      "start state: [ 1  8  1  2  0  0 10  1  1  1  0  3  3 17] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 1  8  1  2  0  0 10  1  1  1  0  3  3 17] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 1  8  1  2  0  0 10  1  1  1  0  3  3 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 401\n",
      "count: 97\n",
      "start state: [ 1  8  1  2  0  0 10  1  1  1  0  3  3 17] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 1  8  1  2  0  0 10  1  1  1  0  3  3 17] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  9  1  2  0  0 10  1  1  1  0  3  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 402\n",
      "count: 98\n",
      "start state: [ 2  9  1  2  0  0 10  1  1  1  0  3  0 18] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 2  9  1  2  0  0 10  1  1  1  0  3  0 18] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  9  1  2  0  0 10  0  2  1  0  3  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 403\n",
      "count: 99\n",
      "start state: [ 2  9  1  2  0  0 10  0  2  1  0  3  0 18] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 2  9  1  2  0  0 10  0  2  1  0  3  0 18] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  9  1  2  0  0 10  0  2  1  0  3  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 404\n",
      "count: 100\n",
      "start state: [ 2  9  1  2  0  0 10  0  2  1  0  3  0 18] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 2  9  1  2  0  0 10  0  2  1  0  3  0 18] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  9  1  2  0  0 10  0  2  1  0  3  0 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "EPISODE 4\n",
      "[4 4 4 4 4 4 0 4 4 4 4 4 4 0]\n",
      "global step: 405\n",
      "count: 0\n",
      "start state: [4 4 4 4 4 4 0 4 4 4 4 4 4 0] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [4 4 4 4 0 5 1 5 5 4 4 4 4 0] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [4 4 4 4 0 5 1 5 0 5 5 5 5 1] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 406\n",
      "count: 1\n",
      "start state: [4 4 4 4 0 5 1 5 0 5 5 5 5 1] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [4 4 4 4 0 5 1 5 0 5 5 5 5 1] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [5 5 5 4 0 5 1 5 0 5 5 0 6 2] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 407\n",
      "count: 2\n",
      "start state: [5 5 5 4 0 5 1 5 0 5 5 0 6 2] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [5 5 5 4 0 5 1 5 0 5 5 0 6 2] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [6 5 5 4 0 5 1 5 0 0 6 1 7 3] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 408\n",
      "count: 3\n",
      "start state: [6 5 5 4 0 5 1 5 0 0 6 1 7 3] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [6 5 5 4 0 5 1 5 0 0 6 1 7 3] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [6 5 5 4 0 5 1 5 0 0 6 1 7 3] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 409\n",
      "count: 4\n",
      "start state: [6 5 5 4 0 5 1 5 0 0 6 1 7 3] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [6 5 5 4 0 5 1 5 0 0 6 1 7 3] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [6 5 5 4 0 5 1 5 0 0 6 1 7 3] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 410\n",
      "count: 5\n",
      "start state: [6 5 5 4 0 5 1 5 0 0 6 1 7 3] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [6 5 5 4 0 5 1 5 0 0 6 1 7 3] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [7 6 6 5 1 6 1 5 0 0 6 1 0 4] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 411\n",
      "count: 6\n",
      "start state: [7 6 6 5 1 6 1 5 0 0 6 1 0 4] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [7 6 6 5 0 7 1 5 0 0 6 1 0 4] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [7 6 6 5 0 7 1 5 0 0 6 0 1 4] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 412\n",
      "count: 7\n",
      "start state: [7 6 6 5 0 7 1 5 0 0 6 0 1 4] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [7 6 6 5 0 7 1 5 0 0 6 0 1 4] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [8 7 7 5 0 7 1 5 0 0 0 1 2 5] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 413\n",
      "count: 8\n",
      "start state: [8 7 7 5 0 7 1 5 0 0 0 1 2 5] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [8 7 7 5 0 0 2 6 1 1 1 2 3 5] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [8 7 7 5 0 0 2 6 1 1 1 0 4 6] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 414\n",
      "count: 9\n",
      "start state: [8 7 7 5 0 0 2 6 1 1 1 0 4 6] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [0 8 8 6 1 1 3 7 2 1 1 0 4 6] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [0 8 8 6 1 1 3 7 2 1 1 0 4 6] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 415\n",
      "count: 10\n",
      "start state: [0 8 8 6 1 1 3 7 2 1 1 0 4 6] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [0 8 8 6 0 2 3 7 2 1 1 0 4 6] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [1 9 9 6 0 2 3 7 2 1 1 0 0 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 416\n",
      "count: 11\n",
      "start state: [1 9 9 6 0 2 3 7 2 1 1 0 0 7] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [1 9 9 6 0 0 4 8 2 1 1 0 0 7] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [1 9 9 6 0 0 4 8 2 1 0 1 0 7] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 417\n",
      "count: 12\n",
      "start state: [1 9 9 6 0 0 4 8 2 1 0 1 0 7] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [1 9 9 6 0 0 4 8 2 1 0 1 0 7] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2 10  9  6  0  0  4  0  3  2  1  2  1  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 418\n",
      "count: 13\n",
      "start state: [ 2 10  9  6  0  0  4  0  3  2  1  2  1  8] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0 11 10  6  0  0  4  0  3  2  1  2  1  8] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0 11 10  6  0  0  4  0  0  3  2  3  1  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 419\n",
      "count: 14\n",
      "start state: [ 0 11 10  6  0  0  4  0  0  3  2  3  1  8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0 11 10  6  0  0  4  0  0  3  2  3  1  8] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0 11 10  6  0  0  4  0  0  0  3  4  2  8] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 420\n",
      "count: 15\n",
      "start state: [ 0 11 10  6  0  0  4  0  0  0  3  4  2  8] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0 11 10  6  0  0  4  0  0  0  3  4  2  8] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1 12 10  6  0  0  4  0  0  0  3  0  3  9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 421\n",
      "count: 16\n",
      "start state: [ 1 12 10  6  0  0  4  0  0  0  3  0  3  9] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 2  0 11  7  1  1  5  1  1  1  4  1  4  9] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  0 11  7  1  1  5  1  1  1  4  0  5  9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 422\n",
      "count: 17\n",
      "start state: [ 2  0 11  7  1  1  5  1  1  1  4  0  5  9] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  1 12  7  1  1  5  1  1  1  4  0  5  9] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  1 12  7  1  1  5  1  1  0  5  0  5  9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 423\n",
      "count: 18\n",
      "start state: [ 0  1 12  7  1  1  5  1  1  0  5  0  5  9] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  1 12  7  1  1  5  1  1  0  5  0  5  9] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  1 12  7  1  1  5  1  1  0  5  0  5  9] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 424\n",
      "count: 19\n",
      "start state: [ 0  1 12  7  1  1  5  1  1  0  5  0  5  9] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  1 12  7  1  1  5  1  1  0  5  0  5  9] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 1  2 13  8  1  1  5  1  1  0  5  0  0 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 425\n",
      "count: 20\n",
      "start state: [ 1  2 13  8  1  1  5  1  1  0  5  0  0 10] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0 14  9  1  1  5  1  1  0  5  0  0 10] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  0 14  9  1  1  5  0  2  0  5  0  0 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 426\n",
      "count: 21\n",
      "start state: [ 1  0 14  9  1  1  5  0  2  0  5  0  0 10] (sum = 48 )\n",
      "learner action: 1\n",
      "intermediate state: [ 1  0 14  9  1  1  5  0  2  0  5  0  0 10] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  0 14  9  1  1  5  0  0  1  6  0  0 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 427\n",
      "count: 22\n",
      "start state: [ 1  0 14  9  1  1  5  0  0  1  6  0  0 10] (sum = 48 )\n",
      "learner action: 3\n",
      "intermediate state: [ 1  0 14  0  2  2  6  1  1  2  7  1  1 10] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  0 14  0  2  2  6  0  2  2  7  1  1 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 428\n",
      "count: 23\n",
      "start state: [ 1  0 14  0  2  2  6  0  2  2  7  1  1 10] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0 14  0  2  2  6  0  2  2  7  1  1 10] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  0 14  0  2  2  6  0  2  2  7  1  1 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 429\n",
      "count: 24\n",
      "start state: [ 1  0 14  0  2  2  6  0  2  2  7  1  1 10] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0 14  0  2  2  6  0  2  2  7  1  1 10] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 1  0 14  0  2  2  6  0  2  0  8  2  1 10] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 430\n",
      "count: 25\n",
      "start state: [ 1  0 14  0  2  2  6  0  2  0  8  2  1 10] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0 14  0  2  2  6  0  2  0  8  2  1 10] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 1  0 14  0  2  2  6  0  2  0  8  2  0 11] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 431\n",
      "count: 26\n",
      "start state: [ 1  0 14  0  2  2  6  0  2  0  8  2  0 11] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0 14  0  2  2  6  0  2  0  8  2  0 11] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1  0 14  0  2  2  6  0  2  0  8  0  1 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 432\n",
      "count: 27\n",
      "start state: [ 1  0 14  0  2  2  6  0  2  0  8  0  1 12] (sum = 48 )\n",
      "learner action: 1\n",
      "intermediate state: [ 1  0 14  0  2  2  6  0  2  0  8  0  1 12] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1  0 14  0  2  2  6  0  2  0  8  0  1 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 433\n",
      "count: 28\n",
      "start state: [ 1  0 14  0  2  2  6  0  2  0  8  0  1 12] (sum = 48 )\n",
      "learner action: 2\n",
      "intermediate state: [ 2  1  1  2  3  3  7  1  3  1  9  1  2 12] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  1  1  2  3  3  7  0  4  1  9  1  2 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 434\n",
      "count: 29\n",
      "start state: [ 2  1  1  2  3  3  7  0  4  1  9  1  2 12] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 2  1  1  2  0  4  8  1  4  1  9  1  2 12] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 2  1  1  2  0  4  8  0  5  1  9  1  2 12] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 435\n",
      "count: 30\n",
      "start state: [ 2  1  1  2  0  4  8  0  5  1  9  1  2 12] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 2  1  1  2  0  4  8  0  5  1  9  1  2 12] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 3  2  2  3  1  5  8  0  5  1  0  2  3 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 436\n",
      "count: 31\n",
      "start state: [ 3  2  2  3  1  5  8  0  5  1  0  2  3 13] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 3  2  0  4  2  5  8  0  5  1  0  2  3 13] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 3  2  0  4  2  5  8  0  5  0  1  2  3 13] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 437\n",
      "count: 32\n",
      "start state: [ 3  2  0  4  2  5  8  0  5  0  1  2  3 13] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 3  2  0  4  2  5  8  0  5  0  1  2  3 13] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 3  2  0  4  2  5  8  0  5  0  1  0  4 14] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 438\n",
      "count: 33\n",
      "start state: [ 3  2  0  4  2  5  8  0  5  0  1  0  4 14] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 3  2  0  4  2  5  8  0  5  0  1  0  4 14] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 3  2  0  4  2  5  8  0  0  1  2  1  5 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 439\n",
      "count: 34\n",
      "start state: [ 3  2  0  4  2  5  8  0  0  1  2  1  5 15] (sum = 48 )\n",
      "learner action: 2\n",
      "intermediate state: [ 3  2  0  4  2  5  8  0  0  1  2  1  5 15] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 3  2  0  4  2  5  8  0  0  1  2  0  6 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 440\n",
      "count: 35\n",
      "start state: [ 3  2  0  4  2  5  8  0  0  1  2  0  6 15] (sum = 48 )\n",
      "learner action: 3\n",
      "intermediate state: [ 3  2  0  0  3  6  9  1  0  1  2  0  6 15] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 3  2  0  0  3  6  9  1  0  1  2  0  6 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 441\n",
      "count: 36\n",
      "start state: [ 3  2  0  0  3  6  9  1  0  1  2  0  6 15] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 3  2  0  0  0  7 10  2  0  1  2  0  6 15] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 3  2  0  0  0  7 10  2  0  1  0  1  7 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 442\n",
      "count: 37\n",
      "start state: [ 3  2  0  0  0  7 10  2  0  1  0  1  7 15] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 3  2  0  0  0  7 10  2  0  1  0  1  7 15] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 3  2  0  0  0  7 10  2  0  1  0  0  8 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 443\n",
      "count: 38\n",
      "start state: [ 3  2  0  0  0  7 10  2  0  1  0  0  8 15] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 3  2  0  0  0  7 10  2  0  1  0  0  8 15] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 3  2  0  0  0  7 10  2  0  1  0  0  8 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 444\n",
      "count: 39\n",
      "start state: [ 3  2  0  0  0  7 10  2  0  1  0  0  8 15] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 3  2  0  0  0  7 10  2  0  1  0  0  8 15] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 3  2  0  0  0  7 10  0  1  2  0  0  8 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 445\n",
      "count: 40\n",
      "start state: [ 3  2  0  0  0  7 10  0  1  2  0  0  8 15] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 3  2  0  0  0  7 10  0  1  2  0  0  8 15] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 3  2  0  0  0  7 10  0  1  2  0  0  8 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 446\n",
      "count: 41\n",
      "start state: [ 3  2  0  0  0  7 10  0  1  2  0  0  8 15] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 3  2  0  0  0  7 10  0  1  2  0  0  8 15] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 3  2  0  0  0  7 10  0  1  0  1  1  8 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 447\n",
      "count: 42\n",
      "start state: [ 3  2  0  0  0  7 10  0  1  0  1  1  8 15] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 3  2  0  0  0  7 10  0  1  0  1  1  8 15] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 3  2  0  0  0  7 10  0  0  1  1  1  8 15] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 448\n",
      "count: 43\n",
      "start state: [ 3  2  0  0  0  7 10  0  0  1  1  1  8 15] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 3  2  0  0  0  7 10  0  0  1  1  1  8 15] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 4  3  1  1  1  8 10  1  0  1  1  1  0 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 449\n",
      "count: 44\n",
      "start state: [ 4  3  1  1  1  8 10  1  0  1  1  1  0 16] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 4  0  2  2  2  8 10  1  0  1  1  1  0 16] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 4  0  2  2  2  8 10  1  0  1  0  2  0 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 450\n",
      "count: 45\n",
      "start state: [ 4  0  2  2  2  8 10  1  0  1  0  2  0 16] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  1  3  3  3  8 10  1  0  1  0  2  0 16] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  1  3  3  3  8 10  1  0  1  0  2  0 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 451\n",
      "count: 46\n",
      "start state: [ 0  1  3  3  3  8 10  1  0  1  0  2  0 16] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 1  1  3  3  3  0 11  2  1  2  1  3  1 16] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  1  3  3  3  0 11  0  2  3  1  3  1 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 452\n",
      "count: 47\n",
      "start state: [ 1  1  3  3  3  0 11  0  2  3  1  3  1 16] (sum = 48 )\n",
      "learner action: 5\n",
      "intermediate state: [ 1  1  3  3  3  0 11  0  2  3  1  3  1 16] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 1  1  3  3  3  0 11  0  2  3  0  4  1 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 453\n",
      "count: 48\n",
      "start state: [ 1  1  3  3  3  0 11  0  2  3  0  4  1 16] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 1  1  3  3  0  1 12  1  2  3  0  4  1 16] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  1  3  3  0  1 12  0  3  3  0  4  1 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 454\n",
      "count: 49\n",
      "start state: [ 1  1  3  3  0  1 12  0  3  3  0  4  1 16] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  1  3  0  1  2 13  0  3  3  0  4  1 16] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  1  3  0  1  2 13  0  0  4  1  5  1 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 455\n",
      "count: 50\n",
      "start state: [ 1  1  3  0  1  2 13  0  0  4  1  5  1 16] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  1  3  0  1  2 13  0  0  4  1  5  1 16] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  1  3  0  1  2 13  0  0  4  1  5  1 16] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 456\n",
      "count: 51\n",
      "start state: [ 1  1  3  0  1  2 13  0  0  4  1  5  1 16] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 1  1  3  0  1  2 13  0  0  4  1  5  1 16] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  2  4  0  1  2 13  0  0  4  1  0  2 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 457\n",
      "count: 52\n",
      "start state: [ 2  2  4  0  1  2 13  0  0  4  1  0  2 17] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  3  5  0  1  2 13  0  0  4  1  0  2 17] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  3  5  0  1  2 13  0  0  4  1  0  2 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 458\n",
      "count: 53\n",
      "start state: [ 0  3  5  0  1  2 13  0  0  4  1  0  2 17] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  3  5  0  1  2 13  0  0  4  1  0  2 17] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  3  5  0  1  2 13  0  0  4  1  0  2 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 459\n",
      "count: 54\n",
      "start state: [ 0  3  5  0  1  2 13  0  0  4  1  0  2 17] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  3  5  0  1  2 13  0  0  4  1  0  2 17] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  3  5  0  1  2 13  0  0  4  0  1  2 17] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 460\n",
      "count: 55\n",
      "start state: [ 0  3  5  0  1  2 13  0  0  4  0  1  2 17] (sum = 48 )\n",
      "learner action: 2\n",
      "intermediate state: [ 0  3  0  1  2  3 14  1  0  4  0  1  2 17] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  3  0  1  2  3 14  1  0  0  1  2  3 18] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 461\n",
      "count: 56\n",
      "start state: [ 0  3  0  1  2  3 14  1  0  0  1  2  3 18] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 0  3  0  1  0  4 15  1  0  0  1  2  3 18] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  3  0  1  0  4 15  1  0  0  1  0  4 19] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 462\n",
      "count: 57\n",
      "start state: [ 0  3  0  1  0  4 15  1  0  0  1  0  4 19] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  1  2  1  4 15  1  0  0  1  0  4 19] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  0  1  2  1  4 15  0  1  0  1  0  4 19] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 463\n",
      "count: 58\n",
      "start state: [ 0  0  1  2  1  4 15  0  1  0  1  0  4 19] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  1  2  1  4 15  0  1  0  1  0  4 19] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  0  1  2  1  4 15  0  1  0  0  1  4 19] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 464\n",
      "count: 59\n",
      "start state: [ 0  0  1  2  1  4 15  0  1  0  0  1  4 19] (sum = 48 )\n",
      "learner action: 3\n",
      "intermediate state: [ 0  0  1  0  2  5 15  0  1  0  0  1  4 19] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  0  1  0  2  5 15  0  1  0  0  1  4 19] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 465\n",
      "count: 60\n",
      "start state: [ 0  0  1  0  2  5 15  0  1  0  0  1  4 19] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  1  0  2  5 15  0  1  0  0  1  4 19] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  0  1  0  2  5 15  0  1  0  0  1  4 19] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 466\n",
      "count: 61\n",
      "start state: [ 0  0  1  0  2  5 15  0  1  0  0  1  4 19] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  1  0  2  5 15  0  1  0  0  1  4 19] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  0  1  0  2  5 15  0  1  0  0  0  5 19] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 467\n",
      "count: 62\n",
      "start state: [ 0  0  1  0  2  5 15  0  1  0  0  0  5 19] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  1  0  2  5 15  0  1  0  0  0  5 19] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  0  1  0  2  5 15  0  1  0  0  0  5 19] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 468\n",
      "count: 63\n",
      "start state: [ 0  0  1  0  2  5 15  0  1  0  0  0  5 19] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  1  0  2  5 15  0  1  0  0  0  5 19] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  0  1  0  2  5 15  0  1  0  0  0  5 19] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 469\n",
      "count: 64\n",
      "start state: [ 0  0  1  0  2  5 15  0  1  0  0  0  5 19] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  1  0  2  5 15  0  1  0  0  0  5 19] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  0  1  0  2  5 15  0  1  0  0  0  5 19] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 470\n",
      "count: 65\n",
      "start state: [ 0  0  1  0  2  5 15  0  1  0  0  0  5 19] (sum = 48 )\n",
      "learner action: 0\n",
      "intermediate state: [ 0  0  1  0  2  5 15  0  1  0  0  0  5 19] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  0  1  0  2  5 15  0  1  0  0  0  5 19] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 471\n",
      "count: 66\n",
      "start state: [ 0  0  1  0  2  5 15  0  1  0  0  0  5 19] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  1  0  2  5 15  0  1  0  0  0  5 19] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 1  1  2  1  2  5 15  0  1  0  0  0  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 472\n",
      "count: 67\n",
      "start state: [ 1  1  2  1  2  5 15  0  1  0  0  0  0 20] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 1  1  2  1  2  0 16  1  2  1  1  0  0 20] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1  1  2  1  2  0 16  1  2  1  1  0  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 473\n",
      "count: 68\n",
      "start state: [ 1  1  2  1  2  0 16  1  2  1  1  0  0 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  2  2  1  2  0 16  1  2  1  1  0  0 20] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  2  2  1  2  0 16  1  2  1  1  0  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 474\n",
      "count: 69\n",
      "start state: [ 0  2  2  1  2  0 16  1  2  1  1  0  0 20] (sum = 48 )\n",
      "learner action: tensor(0)\n",
      "intermediate state: [ 0  2  2  1  2  0 16  1  2  1  1  0  0 20] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  2  2  1  2  0 16  1  0  2  2  0  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 475\n",
      "count: 70\n",
      "start state: [ 0  2  2  1  2  0 16  1  0  2  2  0  0 20] (sum = 48 )\n",
      "learner action: tensor(4)\n",
      "intermediate state: [ 0  2  2  1  0  1 17  1  0  2  2  0  0 20] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  2  2  1  0  1 17  0  1  2  2  0  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 476\n",
      "count: 71\n",
      "start state: [ 0  2  2  1  0  1 17  0  1  2  2  0  0 20] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  3  2  0  1 17  0  1  2  2  0  0 20] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  0  3  2  0  1 17  0  0  3  2  0  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 477\n",
      "count: 72\n",
      "start state: [ 0  0  3  2  0  1 17  0  0  3  2  0  0 20] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  0  3  2  0  0 18  0  0  3  2  0  0 20] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0  0  3  2  0  0 18  0  0  3  2  0  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 478\n",
      "count: 73\n",
      "start state: [ 0  0  3  2  0  0 18  0  0  3  2  0  0 20] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  0  3  2  0  0 18  0  0  3  2  0  0 20] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  0  3  2  0  0 18  0  0  3  2  0  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 479\n",
      "count: 74\n",
      "start state: [ 0  0  3  2  0  0 18  0  0  3  2  0  0 20] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  0  3  2  0  0 18  0  0  3  2  0  0 20] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  0  3  2  0  0 18  0  0  3  2  0  0 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 480\n",
      "count: 75\n",
      "start state: [ 0  0  3  2  0  0 18  0  0  3  2  0  0 20] (sum = 48 )\n",
      "learner action: 4\n",
      "intermediate state: [ 0  0  3  2  0  0 18  0  0  3  2  0  0 20] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 0  0  3  2  0  0 18  0  0  3  0  1  1 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 481\n",
      "count: 76\n",
      "start state: [ 0  0  3  2  0  0 18  0  0  3  0  1  1 20] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  0  3  0  1  1 18  0  0  3  0  1  1 20] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  0  3  0  1  1 18  0  0  3  0  1  1 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 482\n",
      "count: 77\n",
      "start state: [ 0  0  3  0  1  1 18  0  0  3  0  1  1 20] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  0  3  0  1  1 18  0  0  3  0  1  1 20] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  0  3  0  1  1 18  0  0  3  0  1  1 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 483\n",
      "count: 78\n",
      "start state: [ 0  0  3  0  1  1 18  0  0  3  0  1  1 20] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  0  3  0  1  1 18  0  0  3  0  1  1 20] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  0  3  0  1  1 18  0  0  3  0  1  1 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 484\n",
      "count: 79\n",
      "start state: [ 0  0  3  0  1  1 18  0  0  3  0  1  1 20] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  0  3  0  1  1 18  0  0  3  0  1  1 20] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  0  3  0  1  1 18  0  0  3  0  1  1 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 485\n",
      "count: 80\n",
      "start state: [ 0  0  3  0  1  1 18  0  0  3  0  1  1 20] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  0  3  0  1  1 18  0  0  3  0  1  1 20] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  0  3  0  1  1 18  0  0  3  0  1  1 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 486\n",
      "count: 81\n",
      "start state: [ 0  0  3  0  1  1 18  0  0  3  0  1  1 20] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  0  3  0  1  1 18  0  0  3  0  1  1 20] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  0  3  0  1  1 18  0  0  3  0  0  2 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 487\n",
      "count: 82\n",
      "start state: [ 0  0  3  0  1  1 18  0  0  3  0  0  2 20] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  0  3  0  1  1 18  0  0  3  0  0  2 20] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  0  3  0  1  1 18  0  0  3  0  0  2 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 488\n",
      "count: 83\n",
      "start state: [ 0  0  3  0  1  1 18  0  0  3  0  0  2 20] (sum = 48 )\n",
      "learner action: tensor(3)\n",
      "intermediate state: [ 0  0  3  0  1  1 18  0  0  3  0  0  2 20] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 0  0  3  0  1  1 18  0  0  3  0  0  2 20] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 489\n",
      "count: 84\n",
      "start state: [ 0  0  3  0  1  1 18  0  0  3  0  0  2 20] (sum = 48 )\n",
      "learner action: 1\n",
      "intermediate state: [ 0  0  3  0  1  1 18  0  0  3  0  0  2 20] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 1  0  3  0  1  1 18  0  0  3  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 490\n",
      "count: 85\n",
      "start state: [ 1  0  3  0  1  1 18  0  0  3  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 1  0  3  0  1  0 19  0  0  3  0  0  0 21] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 1  0  3  0  1  0 19  0  0  3  0  0  0 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 491\n",
      "count: 86\n",
      "start state: [ 1  0  3  0  1  0 19  0  0  3  0  0  0 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 1  0  3  0  1  0 19  0  0  3  0  0  0 21] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 1  0  3  0  1  0 19  0  0  0  1  1  1 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 492\n",
      "count: 87\n",
      "start state: [ 1  0  3  0  1  0 19  0  0  0  1  1  1 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 1  0  3  0  1  0 19  0  0  0  1  1  1 21] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 1  0  3  0  1  0 19  0  0  0  1  1  1 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 493\n",
      "count: 88\n",
      "start state: [ 1  0  3  0  1  0 19  0  0  0  1  1  1 21] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 1  0  3  0  1  0 19  0  0  0  1  1  1 21] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 1  0  3  0  1  0 19  0  0  0  1  0  2 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 494\n",
      "count: 89\n",
      "start state: [ 1  0  3  0  1  0 19  0  0  0  1  0  2 21] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0  3  0  1  0 19  0  0  0  1  0  2 21] (sum = 48 )\n",
      "opponent action: 8\n",
      "end state: [ 1  0  3  0  1  0 19  0  0  0  1  0  2 21] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 495\n",
      "count: 90\n",
      "start state: [ 1  0  3  0  1  0 19  0  0  0  1  0  2 21] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 1  0  3  0  1  0 19  0  0  0  1  0  2 21] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 2  0  3  0  1  0 19  0  0  0  1  0  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 496\n",
      "count: 91\n",
      "start state: [ 2  0  3  0  1  0 19  0  0  0  1  0  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  0  0  1  2  1 19  0  0  0  1  0  0 22] (sum = 48 )\n",
      "opponent action: 10\n",
      "end state: [ 2  0  0  1  2  1 19  0  0  0  0  1  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 497\n",
      "count: 92\n",
      "start state: [ 2  0  0  1  2  1 19  0  0  0  0  1  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  0  0  1  2  1 19  0  0  0  0  1  0 22] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 2  0  0  1  2  1 19  0  0  0  0  1  0 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 498\n",
      "count: 93\n",
      "start state: [ 2  0  0  1  2  1 19  0  0  0  0  1  0 22] (sum = 48 )\n",
      "learner action: tensor(2)\n",
      "intermediate state: [ 2  0  0  1  2  1 19  0  0  0  0  1  0 22] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 2  0  0  1  2  1 19  0  0  0  0  0  1 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 499\n",
      "count: 94\n",
      "start state: [ 2  0  0  1  2  1 19  0  0  0  0  0  1 22] (sum = 48 )\n",
      "learner action: 0\n",
      "intermediate state: [ 0  1  1  1  2  1 19  0  0  0  0  0  1 22] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  1  1  1  2  1 19  0  0  0  0  0  1 22] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 500\n",
      "count: 95\n",
      "start state: [ 0  1  1  1  2  1 19  0  0  0  0  0  1 22] (sum = 48 )\n",
      "learner action: tensor(1)\n",
      "intermediate state: [ 0  0  2  1  2  1 19  0  0  0  0  0  1 22] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0  0  2  1  2  1 19  0  0  0  0  0  0 23] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 501\n",
      "count: 96\n",
      "start state: [ 0  0  2  1  2  1 19  0  0  0  0  0  0 23] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  0  2  1  2  0 20  0  0  0  0  0  0 23] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  0  2  1  2  0 20  0  0  0  0  0  0 23] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 502\n",
      "count: 97\n",
      "start state: [ 0  0  2  1  2  0 20  0  0  0  0  0  0 23] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  0  2  1  2  0 20  0  0  0  0  0  0 23] (sum = 48 )\n",
      "opponent action: 9\n",
      "end state: [ 0  0  2  1  2  0 20  0  0  0  0  0  0 23] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 503\n",
      "count: 98\n",
      "start state: [ 0  0  2  1  2  0 20  0  0  0  0  0  0 23] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  0  2  1  2  0 20  0  0  0  0  0  0 23] (sum = 48 )\n",
      "opponent action: 7\n",
      "end state: [ 0  0  2  1  2  0 20  0  0  0  0  0  0 23] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 504\n",
      "count: 99\n",
      "start state: [ 0  0  2  1  2  0 20  0  0  0  0  0  0 23] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  0  2  1  2  0 20  0  0  0  0  0  0 23] (sum = 48 )\n",
      "opponent action: 11\n",
      "end state: [ 0  0  2  1  2  0 20  0  0  0  0  0  0 23] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n",
      "global step: 505\n",
      "count: 100\n",
      "start state: [ 0  0  2  1  2  0 20  0  0  0  0  0  0 23] (sum = 48 )\n",
      "learner action: tensor(5)\n",
      "intermediate state: [ 0  0  2  1  2  0 20  0  0  0  0  0  0 23] (sum = 48 )\n",
      "opponent action: 12\n",
      "end state: [ 0  0  2  1  2  0 20  0  0  0  0  0  0 23] (sum = 48 )\n",
      "step reward: 0\n",
      "episode reward: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = GameEnv(100)\n",
    "policy, counts, rewards = train(env, episodes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x318707810>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
